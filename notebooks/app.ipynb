{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import the required libraries and modules\n",
    "from flask import Flask, render_template, request, flash, redirect\n",
    "from werkzeug.utils import secure_filename\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "import chromadb\n",
    "import os\n",
    "import io\n",
    "import pdfminer.high_level\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "import tempfile\n",
    "\n",
    "import shutil"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Import the API-Key from the config file\n",
    "from config import API_KEY\n",
    "\n",
    "# Define the Flask app\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = os.path.join(os.getcwd(), 'uploads')\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "# Define the allowed file types\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf'}\n",
    "\n",
    "# Define the allowed file types\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf'}\n",
    "\n",
    "# Define the function to check if the file extension is allowed\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "# Define the function to generate responses\n",
    "def generate_response(query, chain, chat_history):\n",
    "    # Call the chain function to generate a response\n",
    "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    # Update the chat history with the user's input and the chatbot's response\n",
    "    # chat_history.append((query, result['answer']))\n",
    "\n",
    "    # Return the chatbot's response\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "    # Load the vector database\n",
    "    data_folder = os.path.join(os.getcwd(), 'data')\n",
    "    persist_directory = os.path.join(data_folder, filename)\n",
    "    if os.path.exists(persist_directory):\n",
    "        vectordb = Chroma.from_directory(persist_directory)\n",
    "    else:\n",
    "        # If the file doesn't exist, return None\n",
    "        return None"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/app.py\u001b[0m in \u001b[0;36mline 4\n\u001b[1;32m      <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=57'>58</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=58'>59</a>\u001b[0m \u001b[39m# Load the vector database\u001b[39;00m\n\u001b[1;32m      <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=59'>60</a>\u001b[0m data_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=60'>61</a>\u001b[0m persist_directory \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_folder, filename)\n\u001b[1;32m      <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(persist_directory):\n\u001b[1;32m      <a href='file:///media/gyasis/Blade%2015%20SSD/Users/gyasi/Google%20Drive%20%28not%20syncing%29/Collection/ChatLikeQA/app.py?line=62'>63</a>\u001b[0m     vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_directory(persist_directory)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "filename = 'Oxford-Medical-Handbooks-Tim-Raine-George-Collins-Catriona-Hall-Nina-Hjelde-James-Dawson-Stephan-Sanders-Simon-Eccles-Oxford-Handbook-for-the-Foundation-Programme-Oxford-University-Press-20.pdf'\n",
    "persist_directory = os.path.join(data_folder, filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if os.path.exists(persist_directory):\n",
    "        vectordb = Chroma.from_directory(persist_directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "type(vectordb)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mtype\u001b[39m(vectordb)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "vectordb = Chroma.from_directory('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/Oxford-Medical-Handbooks-Tim-Raine-George-Collins-Catriona-Hall-Nina-Hjelde-James-Dawson-Stephan-Sanders-Simon-Eccles-Oxford-Handbook-for-the-Foundation-Programme-Oxford-University-Press-20.pdf')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'from_directory'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_directory(\u001b[39m'\u001b[39m\u001b[39m/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/Oxford-Medical-Handbooks-Tim-Raine-George-Collins-Catriona-Hall-Nina-Hjelde-James-Dawson-Stephan-Sanders-Simon-Eccles-Oxford-Handbook-for-the-Foundation-Programme-Oxford-University-Press-20.pdf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'from_directory'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dir(Chroma)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " 'add_documents',\n",
       " 'add_texts',\n",
       " 'delete_collection',\n",
       " 'from_documents',\n",
       " 'from_texts',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'persist',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_with_score']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "Chroma.from_disk()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'from_disk'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Chroma\u001b[39m.\u001b[39;49mfrom_disk()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'from_disk'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "Chroma.load()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'load'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Chroma\u001b[39m.\u001b[39;49mload()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'load'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dir(Chroma)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " 'add_documents',\n",
       " 'add_texts',\n",
       " 'delete_collection',\n",
       " 'from_documents',\n",
       " 'from_texts',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'persist',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_with_score']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dir(chromadb)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Client',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__settings',\n",
       " '__spec__',\n",
       " 'chromadb',\n",
       " 'config',\n",
       " 'configure',\n",
       " 'get_db',\n",
       " 'get_settings',\n",
       " 'logging']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "getcwd()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'getcwd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m getcwd()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getcwd' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "client = chromadb.Client()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "client.heartbeat()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1679259053604417545000"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "help(Chroma.from_documents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on method from_documents in module langchain.vectorstores.chroma:\n",
      "\n",
      "from_documents(documents: 'List[Document]', embedding: 'Optional[Embeddings]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, **kwargs: 'Any') -> 'Chroma' method of abc.ABCMeta instance\n",
      "    Create a Chroma vectorstore from a list of documents.\n",
      "    \n",
      "    If a persist_directory is specified, the collection will be persisted there.\n",
      "    Otherwise, the data will be ephemeral in-memory.\n",
      "    \n",
      "    Args:\n",
      "        collection_name (str): Name of the collection to create.\n",
      "        persist_directory (Optional[str]): Directory to persist the collection.\n",
      "        ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      "        documents (List[Document]): List of documents to add to the vectorstore.\n",
      "        embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      "        client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      "    Returns:\n",
      "        Chroma: Chroma vectorstore.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "help(Chroma.add_documents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function add_documents in module langchain.vectorstores.base:\n",
      "\n",
      "add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      "    Run more documents through the embeddings and add to the vectorstore.\n",
      "    \n",
      "    Args:\n",
      "        documents (List[Document]: Documents to add to the vectorstore.\n",
      "    \n",
      "    \n",
      "    Returns:\n",
      "        List[str]: List of IDs of the added texts.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "vectordb"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "loader = UnstructuredFileLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf', strategy='fast')\n",
    "documents = loader.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for doc in docs:\n",
    "    print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "page_content='Compress-Store on Blockchain: A Decentralized\\n\\nData Processing and Immutable Storage for\\n\\nMultimedia Streaming\\n\\nSuayb S. Arslan, Member, IEEE, and Turguy Goker\\n\\n1\\n\\n9\\n\\n1\\n\\n0\\n\\n2\\n\\ny\\n\\na\\n\\nM\\n\\n4\\n\\n2\\n\\n]\\n\\nC\\n\\nD\\n\\n.\\n\\ns\\n\\nc\\n\\n[\\n\\n1\\n\\nv\\n\\n8\\n\\n5\\n\\n4\\n\\n0\\n\\n1\\n\\n.\\n\\n5\\n\\n0\\n\\n9\\n\\n1\\n\\n:\\n\\nv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nAbstract—Decentralization for data storage is a challenging problem for blockchain-based solutions as the blocksize plays the key\\n\\nrole for scalability. In addition, speciﬁc requirements of multimedia data calls for various changes in the blockchain technology internals.\\n\\nConsidering one of the most popular applications of secure multimedia streaming, i.e., video surveillance, it is not clear how to judiciously\\n\\nencode incentivization, immutability and compression into a viable ecosystem. In this study, we provide a genuine scheme that achieves\\n\\nthis encoding for a video surveillance application. The proposed scheme provides a novel integration of data compression, immutable\\n\\noff-chain data storage using a new consensus protocol namely, proof of work storage (PoWS) in order to enable fully useful work to be\\n\\nperformed by the miner nodes of the network. The proposed idea is the ﬁrst step towards achieving greener application of blockchain-\\n\\nbased environment to the video storage business that utilizes system resources efﬁciently.\\n\\nIndex Terms—DLT, Blockchain, data compression, PoW, PoS, multimedia, decentralization.\\n\\n(cid:70)\\n\\n1 INTRODUCTION\\n\\nA CCORDING to recent estimations, there will be over 20\\n\\nbillion connected devices by 2020, all of which will\\n\\ngenerate and then require management, storage, and re-\\n\\ntrieval of large size of data [1]. Connected devices, combined\\n\\nwith consumer-based applications and the increasing need\\n\\nto share data across different business lines, are all playing\\n\\ntheir part in increasing demand for processing and data\\n\\nstorage. Some of these data inherently requires immutabil-\\n\\nity and calls for long term retention. For instance, think\\n\\nabout government archiving or another popular example of\\n\\nvideo/data surveillance. Businesses desiring to launch new,\\n\\ndata-driven applications are bound to confront with incredi-\\n\\nble amount of time, effort and coordination to provision new\\n\\ndatabases today. Now we begin to see dominant commercial\\n\\nand revenue dependency on data which leads to large\\n\\nvolumes being stored in vulnerable centralized databases\\n\\n(even in the cloud), creating privacy and durability risks at\\n\\na scale seldom seen before in history.\\n\\nToday, the dominant practice in unstructured data stor-\\n\\nage is based on a local or remote single system architecture\\n\\nor cloud-based ﬁle/block/object storages (such as Amozon\\n\\nS3 [2] etc.) which are still highly centralized. Although they\\n\\ncan be distributed, they are still in governance of a single\\n\\nbody of management and hence these systems are deﬁnitely\\n\\nconsidered as a beacon for hackers (both external and inter-\\n\\nnal) looking to attack. They also have many points of failure\\n\\nshould the managing company’s ecosystem is affected by an\\n\\nunpredictable system error or experiences down time as a\\n\\nresult of a power outage. In addition, data type being stored\\n\\nhas an immense effect on the management decisions. For\\n\\nexample, multimedia sources are time dependent series of\\n\\n\\n\\nS. S. Arslan is with the Department of Computer Engineering, MEF\\n\\nUniversity, Istanbul, Turkey.\\n\\nE-mail: see http://www.suaybarslan.com/contact.html\\n\\nT. Goker is with adv. dev. lab. of Quantum Corporation.\\n\\ndata and must carefully be protected and communicated\\n\\nby paying attention to streaming requirements. In contrast,\\n\\ndecentralized storage doesnt encounter these problems be-\\n\\ncause it utilizes geographically distributed anonymous or\\n\\npermitted individual nodes, either regionally or globally.\\n\\nHence, meeting point of any applications based on decen-\\n\\ntralized video involves several challenges to tackle. One of\\n\\nthe proven distributed paradigm for' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' storage is known as\\n\\nDistributed Ledger Technology (DLT) [3].\\n\\nDLT can be implemented using different consensus algo-\\n\\nrithms to ascertain that the world view of each node is the\\n\\nsame. Old traditional way is centered around voting-based\\n\\nconsensus such as Paxos [4] then the more understandable\\n\\nversion that goes with the name Raft [5]. Most recently,\\n\\nrandom consensus algorithms have gained popularity. One\\n\\nof the consensus approaches to DLT that became quite com-\\n\\nmon in decentralized cryptocurrency market is blockchain\\n\\n[6]. Considering some open-source public blockchains (such\\n\\nas bitcoin [7] and Ethereum [8]), the set of transactions that\\n\\nare stored within the linked-list of blocks generates a type of\\n\\ndecentralized database or storage of structured data. How-\\n\\never due to scalability concerns, the size of blocks cannot\\n\\ngrow very large and hence it is not hard to see that these\\n\\npublic blockchains are not designed for bulk data storage\\n\\nand management, and using them to do so would consume\\n\\ntoo much local space, too much time for processing and too\\n\\nmuch energy to fulﬁll all the executions. Let us explore some\\n\\nof the decentralized data storage options previously devised\\n\\nand implemented.\\n\\n1.1 Storing data on the blockchain:\\n\\nBlockchains are immutable constructs and hence do not\\n\\nallow random access for write and frequent changes. Also,\\n\\nonly limited number of blocks can be securely added to the\\n\\nchain for a given time period, which makes the throughput\\n\\nCompress-Store on Blockchain: A Decentralized\\n\\nData Processing and Immutable Storage for\\n\\nMultimedia Streaming\\n\\nSuayb S. Arslan, Member, IEEE, and Turguy Goker\\n\\n1\\n\\n9\\n\\n1\\n\\n0\\n\\n2\\n\\ny\\n\\na\\n\\nM\\n\\n4\\n\\n2\\n\\n]\\n\\nC\\n\\nD\\n\\n.\\n\\ns\\n\\nc\\n\\n[\\n\\n1\\n\\nv\\n\\n8\\n\\n5\\n\\n4\\n\\n0\\n\\n1\\n\\n.\\n\\n5\\n\\n0\\n\\n9\\n\\n1\\n\\n:\\n\\nv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nAbstract—Decentralization for data storage is a challenging problem for blockchain-based solutions as the blocksize plays the key\\n\\nrole for scalability. In addition, speciﬁc requirements of multimedia data calls for various changes in the blockchain technology internals.\\n\\nConsidering one of the most popular applications of secure multimedia streaming, i.e., video surveillance, it is not clear how to judiciously\\n\\nencode incentivization, immutability and compression into a viable ecosystem. In this study, we provide a genuine scheme that achieves\\n\\nthis encoding for a video surveillance application. The proposed scheme provides a novel integration of data compression, immutable\\n\\noff-chain data storage using a new consensus protocol namely, proof of work storage (PoWS) in order to enable fully useful work to be\\n\\nperformed by the miner nodes of the network. The proposed idea is the ﬁrst step towards achieving greener application of blockchain-\\n\\nbased environment to the video storage business that utilizes system resources efﬁciently.\\n\\nIndex Terms—DLT, Blockchain, data compression, PoW, PoS, multimedia, decentralization.\\n\\n(cid:70)\\n\\n1 INTRODUCTION\\n\\nA CCORDING to recent estimations, there will be over 20\\n\\nbillion connected devices by 2020, all of which will\\n\\ngenerate and then require management, storage, and re-\\n\\ntrieval of large size of data [1]. Connected devices, combined\\n\\nwith consumer-based applications and the increasing need\\n\\nto share data across different business lines, are all playing\\n\\ntheir part in increasing demand for processing and data\\n\\nstorage. Some of these data inherently requires immutabil-\\n\\nity and calls for long term retention. For instance, think\\n\\nabout government archiving or another popular example of\\n\\nvideo/data surveillance. Businesses desiring to launch new,\\n\\ndata-driven applications are bound to confront with incredi-\\n\\nble amount of time, effort and coordination to provision new\\n\\ndatabases today. Now we begin to see dominant commercial\\n\\nand revenue dependency on data which leads to large\\n\\nvolumes being stored in vulnerable centralized databases\\n\\n(even in the cloud), creating privacy and durability risks at\\n\\na scale seldom seen before in history.\\n\\nToday,' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' the dominant practice in unstructured data stor-\\n\\nage is based on a local or remote single system architecture\\n\\nor cloud-based ﬁle/block/object storages (such as Amozon\\n\\nS3 [2] etc.) which are still highly centralized. Although they\\n\\ncan be distributed, they are still in governance of a single\\n\\nbody of management and hence these systems are deﬁnitely\\n\\nconsidered as a beacon for hackers (both external and inter-\\n\\nnal) looking to attack. They also have many points of failure\\n\\nshould the managing company’s ecosystem is affected by an\\n\\nunpredictable system error or experiences down time as a\\n\\nresult of a power outage. In addition, data type being stored\\n\\nhas an immense effect on the management decisions. For\\n\\nexample, multimedia sources are time dependent series of\\n\\n\\n\\nS. S. Arslan is with the Department of Computer Engineering, MEF\\n\\nUniversity, Istanbul, Turkey.\\n\\nE-mail: see http://www.suaybarslan.com/contact.html\\n\\nT. Goker is with adv. dev. lab. of Quantum Corporation.\\n\\ndata and must carefully be protected and communicated\\n\\nby paying attention to streaming requirements. In contrast,\\n\\ndecentralized storage doesnt encounter these problems be-\\n\\ncause it utilizes geographically distributed anonymous or\\n\\npermitted individual nodes, either regionally or globally.\\n\\nHence, meeting point of any applications based on decen-\\n\\ntralized video involves several challenges to tackle. One of\\n\\nthe proven distributed paradigm for storage is known as\\n\\nDistributed Ledger Technology (DLT) [3].\\n\\nDLT can be implemented using different consensus algo-\\n\\nrithms to ascertain that the world view of each node is the\\n\\nsame. Old traditional way is centered around voting-based\\n\\nconsensus such as Paxos [4] then the more understandable\\n\\nversion that goes with the name Raft [5]. Most recently,\\n\\nrandom consensus algorithms have gained popularity. One\\n\\nof the consensus approaches to DLT that became quite com-\\n\\nmon in decentralized cryptocurrency market is blockchain\\n\\n[6]. Considering some open-source public blockchains (such\\n\\nas bitcoin [7] and Ethereum [8]), the set of transactions that\\n\\nare stored within the linked-list of blocks generates a type of\\n\\ndecentralized database or storage of structured data. How-\\n\\never due to scalability concerns, the size of blocks cannot\\n\\ngrow very large and hence it is not hard to see that these\\n\\npublic blockchains are not designed for bulk data storage\\n\\nand management, and using them to do so would consume\\n\\ntoo much local space, too much time for processing and too\\n\\nmuch energy to fulﬁll all the executions. Let us explore some\\n\\nof the decentralized data storage options previously devised\\n\\nand implemented.\\n\\n1.1 Storing data on the blockchain:\\n\\nBlockchains are immutable constructs and hence do not\\n\\nallow random access for write and frequent changes. Also,\\n\\nonly limited number of blocks can be securely added to the\\n\\nchain for a given time period, which makes the throughput\\n\\n2\\n\\nProject\\n\\nSmart Contracts\\n\\nMulti-region\\n\\nredundancy\\n\\nFeature\\n\\nConsensus\\n\\nScalability (1-3)\\n\\nSia\\n\\nStorj\\n\\nYes\\n\\nNo\\n\\nETH Swarm Yes\\n\\nYes\\n\\nFileCoin\\n\\nNo\\n\\nMaidSafe\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nArchieving,\\n\\nVery decentralized own BC\\n\\nObject,ECC encrypted,\\n\\nsharded, DHT, ETH\\n\\nDHT, ETH\\n\\nIPFS, Replication\\n\\nNo Blockchain\\n\\nBFT\\n\\nProof of Retrievablity\\n\\nProof of Retrievablity\\n\\nProof of Replication\\n\\nClose group consensus\\n\\n2\\n\\n1\\n\\n1\\n\\n1-2\\n\\n3\\n\\nTABLE 1\\n\\nSome decentralized cloud data storage projects centered around distributed technologies. Provided is a rough and relative estimation of scalability\\n\\nusing a range of 1-3. Larger the number is, better scalability it possesses. BC: Blockchain. BFT: Byzantine Fault Tolerance\\n\\nfail to meet most of the data storage requirements. In ad-\\n\\ndition, since the size of the data might be arbitrarily large\\n\\nand full nodes are supposed to store the entire blockchain,\\n\\nthe capacity required storing it will eventually exceed the\\n\\npers' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='istent storage space of many full nodes of the network\\n\\n[9]. Thus, only a speciﬁc set of nodes in the network would\\n\\nbe able to hold the entire blockchain. This in turn will lead\\n\\nto centralization problem i.e., only few nodes will dominate\\n\\nthe database system which in turn will cause loss of security\\n\\nbecause only couple of known and capable nodes will be\\n\\nable to actively participating in the mining process.\\n\\n1.2 Peer-2-peer ﬁle systems:\\n\\nThis approach is based on sharing ﬁles on client computers\\n\\nand uniting them using a global ﬁle system interface. This\\n\\ntechnology utilizes a similar protocol to BitTorrent [10] and\\n\\nDistributed Hash Table (DHT) concepts. Unlike IPs and\\n\\nports, the data contents will be content addressable using\\n\\nhashes of the content allowing separation of storage location\\n\\nand data. Data is available only if the nodes storing the\\n\\ncopies are online. Once the data content is replicated enough\\n\\nnumber of times, the availability/reliability of data is no\\n\\nlonger a concern. DHT-based technology serves only static\\n\\nﬁles which can not be modiﬁed or removed once uploaded.\\n\\nThe deletion of ﬁles cannot be ensured as this technology is\\n\\nnot intended to do so. In other words, the number of copies\\n\\nare not determined by the system but rather the request\\n\\npattern on that data by the network nodes. Lastly, the stored\\n\\nﬁles cannot be searched by their meaningful content. One of\\n\\nthe well known succesful implementations of this idea is\\n\\nknown as InterPlanetary File System (IPFS) [11].\\n\\n1.3 Decentralized Cloud File Storages:\\n\\nMost of these systems resemble to centralized cloud ﬁle\\n\\nstorages such as Dropbox [12]. Peers in the network offer\\n\\ntheir unused persistent storage space for rent and gets\\n\\nrewards in return for providing data storage space and\\n\\nservices. Some of the examples include Sia [13], Storj [14],\\n\\nSwarm [15], Filecoin [16] and MaidSafe [17] which are listed\\n\\nand summarized in Table 1 based on the technologies they\\n\\nare made of. These storage systems provide highly reliable,\\n\\nenormous capacity with varying degrees of access latency\\n\\nand security. As can be seen most of them are based on\\n\\nan implementation of blockchain technology and backed by\\n\\nsome kind of incentivization, so these projects serve static\\n\\nﬁles only, no content search is allowed (unless a speciﬁc\\n\\nfeature gets added as they are all evolving projects) and,\\n\\nsince they are built on peers or anonymous rented hardware,\\n\\nthey are not free of charge. All these projects are optimized\\n\\nfor ﬁle storage (show decent performance with ﬁle accesses)\\n\\nbut fairly fall short in accommodating for time-series data\\n\\n(such as Multimedia or IoT data etc.) An example of such\\n\\ndata include append-only data streams, with a single writer\\n\\nand lots of readers. Although recently few attempts are\\n\\nmade towards creating data storage and sharing ecosystems\\n\\nfor IoT systems, none adequately addresses the streaming\\n\\ndata requirements [18].\\n\\n1.4 Blockchain-based solutions for copyright Protec-\\n\\ntion and Video Hashing:\\n\\nBlockchain technology is very attractive solution for online\\n\\nelectronic notary services, document certiﬁcation, proof of\\n\\nownership and authenticity. Most of such initiatives tar-\\n\\ngeted mobile devices and application development environ-\\n\\nments whereas the blockchain formed the back-end registrar\\n\\nfor document hashes and related information etc. In some\\n\\nof these applications, decentralized database systems are\\n\\npreferred (such as BigChainDB [19] or TiesDB) and the\\n\\nrest use content-addressable decentralized options (such as\\n\\nIPFS). Examples include initiatives such as Block Notary,\\n\\nStampery [20], Verif-y [21]. On the other hand, there are\\n\\nalso available video sharing and video streaming services\\n\\nbased on blockchains [22]. These services verify ownership\\n\\nof each video content as a whole. LIVEPEER for instance\\n\\nis structured for broadcasting by transcoding video source\\n\\ninto all formats and bitrates. Flixxo and Viuly are video\\n\\nsharing platforms [23], in a way competitor projects to\\n\\n' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='Youtube Inc., by offering an entirely decentralized plat-\\n\\nform in which, contrary to their competitor cloud-based\\n\\nproviders, not only content generators are rewarded but\\n\\nalso are the content viewers as well. Viuly is based on\\n\\nEthereum smart contracts and hence do not possess their\\n\\nown blockchain implementation. There are also relatively\\n\\nnew projects which combine different technologies to offer\\n\\nvideo content delivery, sharing, incentivization, security at\\n\\nthe same time (e.g., CoinTube [24]). As a matter of fact,\\n\\nmany of these initiatives can be classiﬁed as one of the\\n\\nfollowing combinations as shown in Figure 1. By choosing\\n\\nan open source project for each layer, one can put together\\n\\na decentralized application (Dapp) and announce an ICO\\n\\neasily if any sort of incentivization is desired.\\n\\nDespite all these new technologies centered around open\\n\\nsource platforms, todays technologies requirements vary at\\n\\na great scale as we move from one application to another.\\n\\nFor instance, we can note that none of these studies\\n\\na. Guarantee the originality of uploaded ﬁles, integrity\\n\\nand authenticity of the video content.\\n\\nb. No veriﬁcation process\\n\\nfor\\n\\nrecorded/uploaded\\n\\nvideos is explicitly deﬁned.\\n\\n2\\n\\nProject\\n\\nSmart Contracts\\n\\nMulti-region\\n\\nredundancy\\n\\nFeature\\n\\nConsensus\\n\\nScalability (1-3)\\n\\nSia\\n\\nStorj\\n\\nYes\\n\\nNo\\n\\nETH Swarm Yes\\n\\nYes\\n\\nFileCoin\\n\\nNo\\n\\nMaidSafe\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nArchieving,\\n\\nVery decentralized own BC\\n\\nObject,ECC encrypted,\\n\\nsharded, DHT, ETH\\n\\nDHT, ETH\\n\\nIPFS, Replication\\n\\nNo Blockchain\\n\\nBFT\\n\\nProof of Retrievablity\\n\\nProof of Retrievablity\\n\\nProof of Replication\\n\\nClose group consensus\\n\\n2\\n\\n1\\n\\n1\\n\\n1-2\\n\\n3\\n\\nTABLE 1\\n\\nSome decentralized cloud data storage projects centered around distributed technologies. Provided is a rough and relative estimation of scalability\\n\\nusing a range of 1-3. Larger the number is, better scalability it possesses. BC: Blockchain. BFT: Byzantine Fault Tolerance\\n\\nfail to meet most of the data storage requirements. In ad-\\n\\ndition, since the size of the data might be arbitrarily large\\n\\nand full nodes are supposed to store the entire blockchain,\\n\\nthe capacity required storing it will eventually exceed the\\n\\npersistent storage space of many full nodes of the network\\n\\n[9]. Thus, only a speciﬁc set of nodes in the network would\\n\\nbe able to hold the entire blockchain. This in turn will lead\\n\\nto centralization problem i.e., only few nodes will dominate\\n\\nthe database system which in turn will cause loss of security\\n\\nbecause only couple of known and capable nodes will be\\n\\nable to actively participating in the mining process.\\n\\n1.2 Peer-2-peer ﬁle systems:\\n\\nThis approach is based on sharing ﬁles on client computers\\n\\nand uniting them using a global ﬁle system interface. This\\n\\ntechnology utilizes a similar protocol to BitTorrent [10] and\\n\\nDistributed Hash Table (DHT) concepts. Unlike IPs and\\n\\nports, the data contents will be content addressable using\\n\\nhashes of the content allowing separation of storage location\\n\\nand data. Data is available only if the nodes storing the\\n\\ncopies are online. Once the data content is replicated enough\\n\\nnumber of times, the availability/reliability of data is no\\n\\nlonger a concern. DHT-based technology serves only static\\n\\nﬁles which can not be modiﬁed or removed once uploaded.\\n\\nThe deletion of ﬁles cannot be ensured as this technology is\\n\\nnot intended to do so. In other words, the number of copies\\n\\nare not determined by the system but rather the request\\n\\npattern on that data by the network nodes. Lastly, the stored\\n\\nﬁles cannot be searched by their meaningful content. One of\\n\\nthe well known succesful implementations of this idea is\\n\\nknown as InterPlanetary File System (IPFS) [11].\\n\\n1.3 Decentralized Cloud File Storages:\\n\\nMost of these systems resemble' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' to centralized cloud ﬁle\\n\\nstorages such as Dropbox [12]. Peers in the network offer\\n\\ntheir unused persistent storage space for rent and gets\\n\\nrewards in return for providing data storage space and\\n\\nservices. Some of the examples include Sia [13], Storj [14],\\n\\nSwarm [15], Filecoin [16] and MaidSafe [17] which are listed\\n\\nand summarized in Table 1 based on the technologies they\\n\\nare made of. These storage systems provide highly reliable,\\n\\nenormous capacity with varying degrees of access latency\\n\\nand security. As can be seen most of them are based on\\n\\nan implementation of blockchain technology and backed by\\n\\nsome kind of incentivization, so these projects serve static\\n\\nﬁles only, no content search is allowed (unless a speciﬁc\\n\\nfeature gets added as they are all evolving projects) and,\\n\\nsince they are built on peers or anonymous rented hardware,\\n\\nthey are not free of charge. All these projects are optimized\\n\\nfor ﬁle storage (show decent performance with ﬁle accesses)\\n\\nbut fairly fall short in accommodating for time-series data\\n\\n(such as Multimedia or IoT data etc.) An example of such\\n\\ndata include append-only data streams, with a single writer\\n\\nand lots of readers. Although recently few attempts are\\n\\nmade towards creating data storage and sharing ecosystems\\n\\nfor IoT systems, none adequately addresses the streaming\\n\\ndata requirements [18].\\n\\n1.4 Blockchain-based solutions for copyright Protec-\\n\\ntion and Video Hashing:\\n\\nBlockchain technology is very attractive solution for online\\n\\nelectronic notary services, document certiﬁcation, proof of\\n\\nownership and authenticity. Most of such initiatives tar-\\n\\ngeted mobile devices and application development environ-\\n\\nments whereas the blockchain formed the back-end registrar\\n\\nfor document hashes and related information etc. In some\\n\\nof these applications, decentralized database systems are\\n\\npreferred (such as BigChainDB [19] or TiesDB) and the\\n\\nrest use content-addressable decentralized options (such as\\n\\nIPFS). Examples include initiatives such as Block Notary,\\n\\nStampery [20], Verif-y [21]. On the other hand, there are\\n\\nalso available video sharing and video streaming services\\n\\nbased on blockchains [22]. These services verify ownership\\n\\nof each video content as a whole. LIVEPEER for instance\\n\\nis structured for broadcasting by transcoding video source\\n\\ninto all formats and bitrates. Flixxo and Viuly are video\\n\\nsharing platforms [23], in a way competitor projects to\\n\\nYoutube Inc., by offering an entirely decentralized plat-\\n\\nform in which, contrary to their competitor cloud-based\\n\\nproviders, not only content generators are rewarded but\\n\\nalso are the content viewers as well. Viuly is based on\\n\\nEthereum smart contracts and hence do not possess their\\n\\nown blockchain implementation. There are also relatively\\n\\nnew projects which combine different technologies to offer\\n\\nvideo content delivery, sharing, incentivization, security at\\n\\nthe same time (e.g., CoinTube [24]). As a matter of fact,\\n\\nmany of these initiatives can be classiﬁed as one of the\\n\\nfollowing combinations as shown in Figure 1. By choosing\\n\\nan open source project for each layer, one can put together\\n\\na decentralized application (Dapp) and announce an ICO\\n\\neasily if any sort of incentivization is desired.\\n\\nDespite all these new technologies centered around open\\n\\nsource platforms, todays technologies requirements vary at\\n\\na great scale as we move from one application to another.\\n\\nFor instance, we can note that none of these studies\\n\\na. Guarantee the originality of uploaded ﬁles, integrity\\n\\nand authenticity of the video content.\\n\\nb. No veriﬁcation process\\n\\nfor\\n\\nrecorded/uploaded\\n\\nvideos is explicitly deﬁned.\\n\\n3\\n\\nblockchain applied to bulk data as well such as chain-\\n\\ning blocks before moving it to off-chain storage. Here are\\n\\nsome properties of the proposed compress-store system;\\n\\n(a) Mining/Consensus is based on the novel Proof-of-\\n\\nWorkStore(PoWS) concept which we will detail later, (b)\\n\\nprocessing/Compression will be decentralized and some\\n\\ncompression related parameters will be stored in blockchain\\n\\nfor later veriﬁcation' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' of recording time, recording place, var-\\n\\nious sensor information and (c) data is selectively chained\\n\\nand encrypted. We will detail these properties of the system\\n\\nnext.\\n\\n2.1 Consensus: Proof of WorkStore (PoWS)\\n\\nBitcoins network uses Proof of Work (PoW) consensus\\n\\nalgorithm in which the blocks are mined by solving a\\n\\nmathematical challenge [28], [7]. This challenge enables the\\n\\nnetwork nodes to reach a consensus and the network in\\n\\nreturn rewards those nodes who participated in system\\n\\nmaintenance and security by offering their CPU resources\\n\\nand solving the challenge. However, there are a couple of\\n\\nproblems with the original concept of PoW:\\n\\n\\n\\nIt\\n\\nleads to exceedingly much and useless en-\\n\\nergy/power consumption. Zero efﬁciency results\\n\\ndue to ﬁnding a solution to a mathematical puzzle\\n\\nthat means no useful work is done. PoW is used to\\n\\nsustain the security of the network run in a public\\n\\ndomain.\\n\\nThe time it takes to show PoW depends on a time-\\n\\ndependent difﬁculty level. This level increases over\\n\\ntime as more miners participate in the ever-growing\\n\\nnetwork. Increased difﬁculty level will lead to block\\n\\nmining process to slow down. Also in order to limit\\n\\nthe chain forking to a minimum, average time be-\\n\\ntween two mining instants is adjusted to meet some\\n\\ncriterion. Such adjustments leads to low transaction\\n\\nthroughput performance as the size of the block (and\\n\\nhence the number of transactions that it contains) is\\n\\nusually not larger than 1MB (in the Bitcoin case - sim-\\n\\nilar sizes apply for other popular public blockchains).\\n\\nIn deﬂationary crypto ecosystems, when mining re-\\n\\nwards cease, only transcation fees will incentivize\\n\\nthe system. Once these fees drop, the number of\\n\\nminers will decline for service leading to unsecure\\n\\nand unprotected system.\\n\\n\\n\\nThe other alternative applicable methods include proof\\n\\nof stake(PoS), proof of Space (PoSpace), proof of Stoage(PoS)\\n\\nand proof of Importance (PoI) etc [29]. and none of which\\n\\nrequire elevated CPU and ASIC requirements for better\\n\\nthroughput performance and green decentralization. In\\n\\nProof of Storage (PoS), miners have to show a proof for\\n\\nenough storage space to store the corresponding data and\\n\\nwill have to guarantee that it never erases data in their\\n\\nlocal or remotely owned storage slots. There are few ways\\n\\nof implementing PoS in literature depending on what is\\n\\nexactly being achieved. Some of PoS schemes include Proof\\n\\nof Retrievability, Provable Data possession, Proof of Replica-\\n\\ntion and the most common implementations to all is to use\\n\\ncryptographic operations and periodic auditing protocols\\n\\n[30].\\n\\nFig. 1. : Layers of Functionality for a decentralized/Incentivized Com-\\n\\nputer System. PoW: Proof of Work, PoS:Proof of Stake. Coin represents\\n\\nsome form of currency used to incentivize the system.\\n\\nc. No supporting proof of time, location, other sensor\\n\\ndata to help the veriﬁcation process of the video\\n\\nauthenticity.\\n\\nd. No genuine immutability (that comprises the full\\n\\ncontent of data) concept other than the linked list\\n\\nof hashing offered by classical blockchains.\\n\\ne. No genuine consensus best ﬁtted for video process-\\n\\ning/surveillance data and applications.\\n\\nTo address some of\\n\\nthese issues, PROVER project\\n\\n(though ICO) and few later publications ( [25]) has recently\\n\\nbe crowdsold and attracted attention since this service ad-\\n\\ndressed a, b and c to some degree. According to PROVER,\\n\\nmobile device users use Swype ID by moving their cell\\n\\nphone in a speciﬁc direction (generated pseudorandomly\\n\\nby the application) to generate code and hashes of the\\n\\ncontent to be stored in the blockchain. PROVER do not care\\n\\nabout where the original content of the data is stored or\\n\\nfor some reason whether it is erased. It is particularly de-\\n\\nsigned for checking authenticity and integrity which alone\\n\\nopens up a wide range of applications including video\\n\\nsurveillance. However, PROVER is powered by Ethereum or\\n\\nN' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='EM blockchains [26] which have their own consensus al-\\n\\ngorithms predetermined and run by their own development\\n\\nenvironments (PoW for Ethereum at the time of writing this\\n\\npaper and PoI: Proof of Importance for NEM). In addition\\n\\nPROVER does treat the video ﬁles as a whole and do not\\n\\nuse its differentiating features that can be combined with\\n\\nblockchain to provide more efﬁcient and useful recording\\n\\nexperience which will contribute to scalability and ﬂexibility\\n\\nof the overall system. In this disclosure, we will be pre-\\n\\nsenting general architectural components when combined\\n\\ntogether will best ﬁt in video streaming and surveillance\\n\\napplications.\\n\\nThe organization of the paper is as follows. In Section 2.,\\n\\nwe introduce the compress-store architecture and provide\\n\\nthe details of the proposed system. In Section 3, we dive\\n\\ninto the details of the implementation and system-level deci-\\n\\nsions. We also provide advantages and disadvantages of the\\n\\nproposed scheme compared to the state-of-the-art. Finally,\\n\\nsection 4 concludes the paper with few future directions.\\n\\n2 COMPRESS-STORE ARCHITECTURE\\n\\nWe propose to use blockchain for metadata (description\\n\\nof which will follow later) storage while the bulk of data\\n\\nis stored off-chain using a distributed hash table system.\\n\\nThe off-chain choice is completely arbitrary and could be\\n\\nreplaced with existing cloud services such as Azure [27]\\n\\nor S3 [2]. However, we provide desirable properties of a\\n\\n3\\n\\nblockchain applied to bulk data as well such as chain-\\n\\ning blocks before moving it to off-chain storage. Here are\\n\\nsome properties of the proposed compress-store system;\\n\\n(a) Mining/Consensus is based on the novel Proof-of-\\n\\nWorkStore(PoWS) concept which we will detail later, (b)\\n\\nprocessing/Compression will be decentralized and some\\n\\ncompression related parameters will be stored in blockchain\\n\\nfor later veriﬁcation of recording time, recording place, var-\\n\\nious sensor information and (c) data is selectively chained\\n\\nand encrypted. We will detail these properties of the system\\n\\nnext.\\n\\n2.1 Consensus: Proof of WorkStore (PoWS)\\n\\nBitcoins network uses Proof of Work (PoW) consensus\\n\\nalgorithm in which the blocks are mined by solving a\\n\\nmathematical challenge [28], [7]. This challenge enables the\\n\\nnetwork nodes to reach a consensus and the network in\\n\\nreturn rewards those nodes who participated in system\\n\\nmaintenance and security by offering their CPU resources\\n\\nand solving the challenge. However, there are a couple of\\n\\nproblems with the original concept of PoW:\\n\\n\\n\\nIt\\n\\nleads to exceedingly much and useless en-\\n\\nergy/power consumption. Zero efﬁciency results\\n\\ndue to ﬁnding a solution to a mathematical puzzle\\n\\nthat means no useful work is done. PoW is used to\\n\\nsustain the security of the network run in a public\\n\\ndomain.\\n\\nThe time it takes to show PoW depends on a time-\\n\\ndependent difﬁculty level. This level increases over\\n\\ntime as more miners participate in the ever-growing\\n\\nnetwork. Increased difﬁculty level will lead to block\\n\\nmining process to slow down. Also in order to limit\\n\\nthe chain forking to a minimum, average time be-\\n\\ntween two mining instants is adjusted to meet some\\n\\ncriterion. Such adjustments leads to low transaction\\n\\nthroughput performance as the size of the block (and\\n\\nhence the number of transactions that it contains) is\\n\\nusually not larger than 1MB (in the Bitcoin case - sim-\\n\\nilar sizes apply for other popular public blockchains).\\n\\nIn deﬂationary crypto ecosystems, when mining re-\\n\\nwards cease, only transcation fees will incentivize\\n\\nthe system. Once these fees drop, the number of\\n\\nminers will decline for service leading to unsecure\\n\\nand unprotected system.\\n\\n\\n\\nThe other alternative applicable methods include proof\\n\\nof stake(PoS), proof of Space (PoSpace), proof of Stoage(PoS)\\n\\nand proof of Importance (PoI) etc [29]. and none of which\\n\\nrequire elevated CPU and ASIC requirements for better\\n\\nthroughput performance and green decentralization.' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' In\\n\\nProof of Storage (PoS), miners have to show a proof for\\n\\nenough storage space to store the corresponding data and\\n\\nwill have to guarantee that it never erases data in their\\n\\nlocal or remotely owned storage slots. There are few ways\\n\\nof implementing PoS in literature depending on what is\\n\\nexactly being achieved. Some of PoS schemes include Proof\\n\\nof Retrievability, Provable Data possession, Proof of Replica-\\n\\ntion and the most common implementations to all is to use\\n\\ncryptographic operations and periodic auditing protocols\\n\\n[30].\\n\\nFig. 1. : Layers of Functionality for a decentralized/Incentivized Com-\\n\\nputer System. PoW: Proof of Work, PoS:Proof of Stake. Coin represents\\n\\nsome form of currency used to incentivize the system.\\n\\nc. No supporting proof of time, location, other sensor\\n\\ndata to help the veriﬁcation process of the video\\n\\nauthenticity.\\n\\nd. No genuine immutability (that comprises the full\\n\\ncontent of data) concept other than the linked list\\n\\nof hashing offered by classical blockchains.\\n\\ne. No genuine consensus best ﬁtted for video process-\\n\\ning/surveillance data and applications.\\n\\nTo address some of\\n\\nthese issues, PROVER project\\n\\n(though ICO) and few later publications ( [25]) has recently\\n\\nbe crowdsold and attracted attention since this service ad-\\n\\ndressed a, b and c to some degree. According to PROVER,\\n\\nmobile device users use Swype ID by moving their cell\\n\\nphone in a speciﬁc direction (generated pseudorandomly\\n\\nby the application) to generate code and hashes of the\\n\\ncontent to be stored in the blockchain. PROVER do not care\\n\\nabout where the original content of the data is stored or\\n\\nfor some reason whether it is erased. It is particularly de-\\n\\nsigned for checking authenticity and integrity which alone\\n\\nopens up a wide range of applications including video\\n\\nsurveillance. However, PROVER is powered by Ethereum or\\n\\nNEM blockchains [26] which have their own consensus al-\\n\\ngorithms predetermined and run by their own development\\n\\nenvironments (PoW for Ethereum at the time of writing this\\n\\npaper and PoI: Proof of Importance for NEM). In addition\\n\\nPROVER does treat the video ﬁles as a whole and do not\\n\\nuse its differentiating features that can be combined with\\n\\nblockchain to provide more efﬁcient and useful recording\\n\\nexperience which will contribute to scalability and ﬂexibility\\n\\nof the overall system. In this disclosure, we will be pre-\\n\\nsenting general architectural components when combined\\n\\ntogether will best ﬁt in video streaming and surveillance\\n\\napplications.\\n\\nThe organization of the paper is as follows. In Section 2.,\\n\\nwe introduce the compress-store architecture and provide\\n\\nthe details of the proposed system. In Section 3, we dive\\n\\ninto the details of the implementation and system-level deci-\\n\\nsions. We also provide advantages and disadvantages of the\\n\\nproposed scheme compared to the state-of-the-art. Finally,\\n\\nsection 4 concludes the paper with few future directions.\\n\\n2 COMPRESS-STORE ARCHITECTURE\\n\\nWe propose to use blockchain for metadata (description\\n\\nof which will follow later) storage while the bulk of data\\n\\nis stored off-chain using a distributed hash table system.\\n\\nThe off-chain choice is completely arbitrary and could be\\n\\nreplaced with existing cloud services such as Azure [27]\\n\\nor S3 [2]. However, we provide desirable properties of a\\n\\nSince the ideal decentralized computer system is ex-\\n\\npected to establish both decentralized computing and stor-\\n\\nage at the same time, it is essential that we provide in our\\n\\nvideo surveillance system (1) Video Processing (compres-\\n\\nsion in our particular case) (2) Data storage (3) Immutability\\n\\nand (4) Security. Video compression has two advantages:\\n\\nﬁrst it requires some form of computation due to video\\n\\nprocessing (transformation, quantization and coding) and\\n\\nthis can be done in a decentralized fashion and yet does\\n\\nnot pose a lot of useless computations as in original PoW\\n\\nand the difﬁculty only changes as new compression al-\\n\\ngorithms and techniques or new quality requirements are\\n' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='\\nintegrated/imposed into/onto the network. This will form\\n\\nthe proof of work part of our consensus, namely PoWS.\\n\\nThe second advantage is that since raw video ﬁles would\\n\\nbe compressed at a speciﬁed quality, we can save a lot of\\n\\nstorage space. This does not only mean that we will be\\n\\nsaving storage resources but also computation resources\\n\\nthat might be due to encryption, digital signature generation\\n\\netc. Note that one other advantage comes at no cost from\\n\\nthe incentivization point of view, because miners may want\\n\\nto choose high compression performance to be able to ﬁnd\\n\\nstorage place quickly and hence be successful at mining\\n\\nprocess. This will lead to total storage of the system to be\\n\\nused wisely.\\n\\nVideo ﬁles are bulky and it is always hard to deal with\\n\\nlarge volumes of data. In the compress-store architecture,\\n\\nthe data storage is provided off-chain using a distributed\\n\\nhash table system. One possible realization is the content-\\n\\naddressed data chunk storage technologies such as IPFS. In\\n\\nthat case the data location is represented by a unique hash\\n\\nand we separate the content location in the network and the\\n\\nIP/port number of the server. However, all location pointers\\n\\nwill be stored in the blockchain. Finally since the metadata\\n\\nis stored in the blockchain, it is immutable and cannot be\\n\\nchanged by any easy means.\\n\\nThe main idea behind inserting some kind of PoW into\\n\\nour system is ﬁrst to dramatically increase the scalability\\n\\nof the system and it would make really hard to generate\\n\\ncompressed sequences of thousands of frames in a relatively\\n\\nshort time (unless application speciﬁc hardware is used.\\n\\nHowever, keep in mind that video encoders comes in great\\n\\nvariety of parameter selections and algorithmic differences)\\n\\nwhich discourages attackers and allows the network to use\\n\\nthe proposed system in the public domain.\\n\\n2.2 Throughput of the Surveillance System\\n\\nIn one application of the proposed idea, miners compress\\n\\nvideo frames. Video ﬁles are typically partitioned into\\n\\nGroup of Pictures (GOP) and each is processed indepdent of\\n\\nthe other. Hence we deﬁne throughput to be the processed\\n\\nframes committed to the blockchain per second. In a typical\\n\\nscenario, a GOP can contain 25 pictures and each block can\\n\\ndescribe around 5 GOPs at the same time. If each block is\\n\\nveriﬁed and added to blockchain every 10 secs, this makes\\n\\n12.5 frames or pictures per second (pps). This is extremely\\n\\nslow rate compared to the level of video generation by\\n\\nthe system in a typical surveillance application. Obvious\\n\\nway to improve throughput is to increase GOP size at the\\n\\nexpense of lesser quality compression and larger storage\\n\\n4\\n\\nrequirement. Another popular way to alleviate this is the\\n\\nmethod of sharding [31] that is also being considered to\\n\\nsolve the scalability issues of popular public blockchains\\n\\nsuch as Ethereum. In that scheme, miners choose a GOP or\\n\\na consecutive group of GOPs pseudorandomly and work on\\n\\ntheir compression workload. This would allow parallelism\\n\\nin the network and hence would ensure better throughput\\n\\nperformance. However implementation of such a scheme\\n\\nmight be a bit tricky.\\n\\nYet, another approach is to make the blockchain private.\\n\\nIn that case, the security requirements will be less of an issue\\n\\ndue to trusted parties and hence more frequent block addi-\\n\\ntions to the chain can be realized. This would eventually\\n\\nlead to better throughput.\\n\\n2.3 Private/Public Blockchain applications\\n\\nIn one private blockchain implementation of the proposed\\n\\nidea, compression workload can completely be handled by\\n\\nthe video generator node that we call initiator node. This\\n\\nprocess can alternatively be handled completely decentral-\\n\\nized if need be. In that case, the PoW part of our system\\n\\ncan be avoided since the participators are assumed to be\\n\\ntrusted parties and pose no risk to the system. This way,\\n\\nthe number of committed video frames into the blockchain\\n\\ncan be increased dramatically and consensus can be reached\\n\\na lot easily. This will eventually increase the throughput of\\n\\nthe system. Although a form of centralization may dominate\\n\\n(also due to a subset of miner node selection process), all\\n\\nother properties of the proposed scheme will still serve\\n\\na number of advantages regarding the' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' video surveillance\\n\\napplications.\\n\\nIn one public blockchain implementation of the pro-\\n\\nposed idea, we shall use PoWS at full scale. Compared to\\n\\nprivate counterpart, there are number of differences in this\\n\\ncase. First, we decentralize the computation by allowing\\n\\nminer nodes to compress and encrypt video frames, ﬁnd an\\n\\nappropriate storage location before preparing and adding\\n\\nthe related metadata into the blockchain. Since these are\\n\\nthird party participators, we propose to incentivize them\\n\\nby coins which will help them process more videos and\\n\\nuse storage space. We also incentivize better compression\\n\\n(avoid dump compression styles) because ﬁnding an ap-\\n\\npropriate storage location and space can only be found\\n\\nthrough paying the required amount using coins. Miner\\n\\nnodes are motivated to use better methods to be able to ask\\n\\nfor less storage space that also meets a predeﬁned (just like\\n\\ndifﬁculty level of a Bitcoin network) quality requirement.\\n\\nThis quality requirement may be updated as the network\\n\\nevolves or more miners participate. We refer the reader to\\n\\nthe mining process for details.\\n\\n3 IMPLEMENTATION AND SYSTEM DETAILS\\n\\nIn this section, we provide the implementation and system-\\n\\nlevel details. some of these details are related to veriﬁcation\\n\\nand data storage phases. Later, we shall compare these\\n\\ndetails with the state-of-the-art.\\n\\nSince the ideal decentralized computer system is ex-\\n\\npected to establish both decentralized computing and stor-\\n\\nage at the same time, it is essential that we provide in our\\n\\nvideo surveillance system (1) Video Processing (compres-\\n\\nsion in our particular case) (2) Data storage (3) Immutability\\n\\nand (4) Security. Video compression has two advantages:\\n\\nﬁrst it requires some form of computation due to video\\n\\nprocessing (transformation, quantization and coding) and\\n\\nthis can be done in a decentralized fashion and yet does\\n\\nnot pose a lot of useless computations as in original PoW\\n\\nand the difﬁculty only changes as new compression al-\\n\\ngorithms and techniques or new quality requirements are\\n\\nintegrated/imposed into/onto the network. This will form\\n\\nthe proof of work part of our consensus, namely PoWS.\\n\\nThe second advantage is that since raw video ﬁles would\\n\\nbe compressed at a speciﬁed quality, we can save a lot of\\n\\nstorage space. This does not only mean that we will be\\n\\nsaving storage resources but also computation resources\\n\\nthat might be due to encryption, digital signature generation\\n\\netc. Note that one other advantage comes at no cost from\\n\\nthe incentivization point of view, because miners may want\\n\\nto choose high compression performance to be able to ﬁnd\\n\\nstorage place quickly and hence be successful at mining\\n\\nprocess. This will lead to total storage of the system to be\\n\\nused wisely.\\n\\nVideo ﬁles are bulky and it is always hard to deal with\\n\\nlarge volumes of data. In the compress-store architecture,\\n\\nthe data storage is provided off-chain using a distributed\\n\\nhash table system. One possible realization is the content-\\n\\naddressed data chunk storage technologies such as IPFS. In\\n\\nthat case the data location is represented by a unique hash\\n\\nand we separate the content location in the network and the\\n\\nIP/port number of the server. However, all location pointers\\n\\nwill be stored in the blockchain. Finally since the metadata\\n\\nis stored in the blockchain, it is immutable and cannot be\\n\\nchanged by any easy means.\\n\\nThe main idea behind inserting some kind of PoW into\\n\\nour system is ﬁrst to dramatically increase the scalability\\n\\nof the system and it would make really hard to generate\\n\\ncompressed sequences of thousands of frames in a relatively\\n\\nshort time (unless application speciﬁc hardware is used.\\n\\nHowever, keep in mind that video encoders comes in great\\n\\nvariety of parameter selections and algorithmic differences)\\n\\nwhich discourages attackers and allows the network to use\\n\\nthe proposed system in the public domain.\\n\\n2.2 Throughput of the Surveillance System\\n\\nIn one application of the proposed idea, miners compress\\n\\nvideo frames. Video ﬁles are typically partitioned into\\n\\nGroup of Pictures (GOP) and each is processed indepdent of\\n\\n' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='the other. Hence we deﬁne throughput to be the processed\\n\\nframes committed to the blockchain per second. In a typical\\n\\nscenario, a GOP can contain 25 pictures and each block can\\n\\ndescribe around 5 GOPs at the same time. If each block is\\n\\nveriﬁed and added to blockchain every 10 secs, this makes\\n\\n12.5 frames or pictures per second (pps). This is extremely\\n\\nslow rate compared to the level of video generation by\\n\\nthe system in a typical surveillance application. Obvious\\n\\nway to improve throughput is to increase GOP size at the\\n\\nexpense of lesser quality compression and larger storage\\n\\n4\\n\\nrequirement. Another popular way to alleviate this is the\\n\\nmethod of sharding [31] that is also being considered to\\n\\nsolve the scalability issues of popular public blockchains\\n\\nsuch as Ethereum. In that scheme, miners choose a GOP or\\n\\na consecutive group of GOPs pseudorandomly and work on\\n\\ntheir compression workload. This would allow parallelism\\n\\nin the network and hence would ensure better throughput\\n\\nperformance. However implementation of such a scheme\\n\\nmight be a bit tricky.\\n\\nYet, another approach is to make the blockchain private.\\n\\nIn that case, the security requirements will be less of an issue\\n\\ndue to trusted parties and hence more frequent block addi-\\n\\ntions to the chain can be realized. This would eventually\\n\\nlead to better throughput.\\n\\n2.3 Private/Public Blockchain applications\\n\\nIn one private blockchain implementation of the proposed\\n\\nidea, compression workload can completely be handled by\\n\\nthe video generator node that we call initiator node. This\\n\\nprocess can alternatively be handled completely decentral-\\n\\nized if need be. In that case, the PoW part of our system\\n\\ncan be avoided since the participators are assumed to be\\n\\ntrusted parties and pose no risk to the system. This way,\\n\\nthe number of committed video frames into the blockchain\\n\\ncan be increased dramatically and consensus can be reached\\n\\na lot easily. This will eventually increase the throughput of\\n\\nthe system. Although a form of centralization may dominate\\n\\n(also due to a subset of miner node selection process), all\\n\\nother properties of the proposed scheme will still serve\\n\\na number of advantages regarding the video surveillance\\n\\napplications.\\n\\nIn one public blockchain implementation of the pro-\\n\\nposed idea, we shall use PoWS at full scale. Compared to\\n\\nprivate counterpart, there are number of differences in this\\n\\ncase. First, we decentralize the computation by allowing\\n\\nminer nodes to compress and encrypt video frames, ﬁnd an\\n\\nappropriate storage location before preparing and adding\\n\\nthe related metadata into the blockchain. Since these are\\n\\nthird party participators, we propose to incentivize them\\n\\nby coins which will help them process more videos and\\n\\nuse storage space. We also incentivize better compression\\n\\n(avoid dump compression styles) because ﬁnding an ap-\\n\\npropriate storage location and space can only be found\\n\\nthrough paying the required amount using coins. Miner\\n\\nnodes are motivated to use better methods to be able to ask\\n\\nfor less storage space that also meets a predeﬁned (just like\\n\\ndifﬁculty level of a Bitcoin network) quality requirement.\\n\\nThis quality requirement may be updated as the network\\n\\nevolves or more miners participate. We refer the reader to\\n\\nthe mining process for details.\\n\\n3 IMPLEMENTATION AND SYSTEM DETAILS\\n\\nIn this section, we provide the implementation and system-\\n\\nlevel details. some of these details are related to veriﬁcation\\n\\nand data storage phases. Later, we shall compare these\\n\\ndetails with the state-of-the-art.\\n\\n3.1 The procedure for Veriﬁcation and Storage\\n\\nIn our compress-store architecture, we deﬁne three types\\n\\nof nodes which have their own way of behavior explained\\n\\nbelow.\\n\\n1.\\n\\nInitiator nodes: These nodes are usually equipped\\n\\nwith video camcorders and are able to modify/edit\\n\\nrecorded video streams. These streams can be di-\\n\\nvided into one or more groups of GOPs. The cap-\\n\\ntured GOPs are usually selected to be small and they\\n\\nconstitute the block of information to be processed\\n\\nby the network.\\n\\n2. Mining nodes: These nodes are equipped with video\\n\\ncompression tools/encoding software or hardware.\\n\\nThey' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' process arrived frames of a GOP and send\\n\\nindentiﬁcation information to the network to ini-\\n\\ntiate veriﬁcation process. Once their work is veri-\\n\\nﬁed, a block representing one GOP is added to the\\n\\nblockchain and they are rewarded with more storage\\n\\nspace to use for mining.\\n\\n3. Storage nodes: These nodes are responsible of stor-\\n\\ning bulk data which is in our case the compressed\\n\\nmultimedia source ﬁles. These ﬁles are stored en-\\n\\ncrypted after compression. Storage nodes run a form\\n\\nof PoS algorithm to make sure that they reserved\\n\\nthe amount of space that they promise to. Although\\n\\nblock veriﬁcation verify the availability of data and\\n\\nthe storage space, a travelling auditing service shall\\n\\nbe used by the network to check this veriﬁcation\\n\\nprocess at a regular basis. Storage nodes receive fees\\n\\nonce they complete all the requirements of the PoS\\n\\nand as long as they store the multimedia data.\\n\\nPhysical nodes of our peer-2-peer network can assume\\n\\nall these three types of node capabilities. For instance, full\\n\\nnodes can initiate video storage, mine frames and store\\n\\ncompressed multimedia data at the same time. On the other\\n\\nhand, veriﬁer nodes are all participating network nodes that\\n\\nstore a copy of the blockchain.\\n\\nThe multimedia data itself\\n\\nis NOT stored in the\\n\\nblockchain. Instead their representative data is maintained\\n\\non-chain serving as pointers to their stored locations. We de-\\n\\nﬁne a subblock to include the following information: 1. The\\n\\ntime GOP is generated, 2. Hash of the GOP (through Merkle\\n\\nroot), 3. Access privileges, 4. The location of stored GOP\\n\\n(this does not need to be a physical address, an alternative\\n\\nis to use content addressing, also geographical location can\\n\\nbe incorporated), 5. Metadata about the stored multimedia\\n\\ncontent such as compression algorithm, parameters, etc. In\\n\\naddition, Variety of sensor information, GOP index and\\n\\norder, video identiﬁcation number/labeling could also be\\n\\npart of the subblock for further veriﬁcation process. These\\n\\nadditional information is important for the reconstruction\\n\\nof the video ﬁles. Once a subblock is formed it is digitally\\n\\nsigned with initiators private key before sending in to net-\\n\\nwork. Miner nodes shall collect enough number of such sub-\\n\\nblocks and the associated multimedia data frames/GOPs\\n\\nto start compression process immediately after such sub-\\n\\nblocks make up a predetermined block size (determined\\n\\nby the overall network). This predetermined block size is\\n\\nanalogous to the block size of other crypto-networks such\\n\\nas Bitcoin.\\n\\n5\\n\\nFig. 2. An example of two GOPs and the procedure of Merkle root\\n\\ncomputation within and across GOPs.\\n\\nFor compression to make sense, an initiator must set a\\n\\nquality measure such as Mean square Error (MSE) or a peak\\n\\nsignal to noise ratio (PSNR) or an another subjective multi-\\n\\nmedia quality indicator that will help identify that two ﬁles\\n\\nare the same except one of them is the compressed version.\\n\\nThere are few technologies/algorithms that identiﬁes two\\n\\nvideo ﬁles to be the same whether they are compressed\\n\\nor not. Miners time to mine a block requires the miner to\\n\\ncomplete the compression process (by going through each\\n\\nsub-compression steps), meet the quality measure require-\\n\\nment, compute the merkle hash tree of a GOP, chain the\\n\\nvideo frames, encrypt the content, ﬁnd a storage node (or his\\n\\nown local resources) which ensures storage space required\\n\\nto store the compressed content (generate a proof). In order\\n\\nto make the size of a block even smaller, we employ Merkle\\n\\nhash tree of GOPs as well. The way the Merkle root is\\n\\ncomputed is shown in Figure 2.\\n\\nOnce this work is done, the mining node broadcasts\\n\\nthe block/s to the network that has all the information\\n\\nabout the GOP except the bulk data itself. All veriﬁer nodes\\n\\nwhich receive this block begin the veriﬁcation process which\\n\\nwould involve:\\n\\na' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='. A comprehensive check whether GOPs are really\\n\\nstored in the designated locations. This requires\\n\\npreparing intelligent challenges for provers (miners)\\n\\nfor PoS.\\n\\nb. A comprehensive check for the merkle hash by re-\\n\\nquesting hashes of the frames and GOPs from stor-\\n\\nage node/s.\\n\\nc. A comprehensive check for the quality measure\\n\\nwhether it meets or not, using the uncompressed\\n\\nGOP data. This is to ensure that the miner nodes\\n\\nare legitimately compressed the multimedia ﬁle. This\\n\\neffort in fact characterizes a form of Proof of Com-\\n\\npression (PoC).\\n\\nOnce veriﬁed, all uncompressed copies are removed\\n\\nfrom veriﬁer node caches to open storage space for the\\n\\nnext uncompressed GOP/s data. As can be observed, since\\n\\nblocks do not include transactions in our case, we keep\\n\\nthe size of the block that contains metadata for GOP/s to\\n\\naround only a fraction of KBs. This is to limit the total\\n\\nsize of the blockchain stored in all of the veriﬁer nodes.\\n\\nOne can realize that as we include more metadata (sensor\\n\\n3.1 The procedure for Veriﬁcation and Storage\\n\\nIn our compress-store architecture, we deﬁne three types\\n\\nof nodes which have their own way of behavior explained\\n\\nbelow.\\n\\n1.\\n\\nInitiator nodes: These nodes are usually equipped\\n\\nwith video camcorders and are able to modify/edit\\n\\nrecorded video streams. These streams can be di-\\n\\nvided into one or more groups of GOPs. The cap-\\n\\ntured GOPs are usually selected to be small and they\\n\\nconstitute the block of information to be processed\\n\\nby the network.\\n\\n2. Mining nodes: These nodes are equipped with video\\n\\ncompression tools/encoding software or hardware.\\n\\nThey process arrived frames of a GOP and send\\n\\nindentiﬁcation information to the network to ini-\\n\\ntiate veriﬁcation process. Once their work is veri-\\n\\nﬁed, a block representing one GOP is added to the\\n\\nblockchain and they are rewarded with more storage\\n\\nspace to use for mining.\\n\\n3. Storage nodes: These nodes are responsible of stor-\\n\\ning bulk data which is in our case the compressed\\n\\nmultimedia source ﬁles. These ﬁles are stored en-\\n\\ncrypted after compression. Storage nodes run a form\\n\\nof PoS algorithm to make sure that they reserved\\n\\nthe amount of space that they promise to. Although\\n\\nblock veriﬁcation verify the availability of data and\\n\\nthe storage space, a travelling auditing service shall\\n\\nbe used by the network to check this veriﬁcation\\n\\nprocess at a regular basis. Storage nodes receive fees\\n\\nonce they complete all the requirements of the PoS\\n\\nand as long as they store the multimedia data.\\n\\nPhysical nodes of our peer-2-peer network can assume\\n\\nall these three types of node capabilities. For instance, full\\n\\nnodes can initiate video storage, mine frames and store\\n\\ncompressed multimedia data at the same time. On the other\\n\\nhand, veriﬁer nodes are all participating network nodes that\\n\\nstore a copy of the blockchain.\\n\\nThe multimedia data itself\\n\\nis NOT stored in the\\n\\nblockchain. Instead their representative data is maintained\\n\\non-chain serving as pointers to their stored locations. We de-\\n\\nﬁne a subblock to include the following information: 1. The\\n\\ntime GOP is generated, 2. Hash of the GOP (through Merkle\\n\\nroot), 3. Access privileges, 4. The location of stored GOP\\n\\n(this does not need to be a physical address, an alternative\\n\\nis to use content addressing, also geographical location can\\n\\nbe incorporated), 5. Metadata about the stored multimedia\\n\\ncontent such as compression algorithm, parameters, etc. In\\n\\naddition, Variety of sensor information, GOP index and\\n\\norder, video identiﬁcation number/labeling could also be\\n\\npart of the subblock for further veriﬁcation process. These\\n\\nadditional information is important for the reconstruction\\n\\nof the video ﬁles. Once a subblock is formed it is digitally\\n\\nsigned with initiators private key before sending in to net-\\n' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='\\nwork. Miner nodes shall collect enough number of such sub-\\n\\nblocks and the associated multimedia data frames/GOPs\\n\\nto start compression process immediately after such sub-\\n\\nblocks make up a predetermined block size (determined\\n\\nby the overall network). This predetermined block size is\\n\\nanalogous to the block size of other crypto-networks such\\n\\nas Bitcoin.\\n\\n5\\n\\nFig. 2. An example of two GOPs and the procedure of Merkle root\\n\\ncomputation within and across GOPs.\\n\\nFor compression to make sense, an initiator must set a\\n\\nquality measure such as Mean square Error (MSE) or a peak\\n\\nsignal to noise ratio (PSNR) or an another subjective multi-\\n\\nmedia quality indicator that will help identify that two ﬁles\\n\\nare the same except one of them is the compressed version.\\n\\nThere are few technologies/algorithms that identiﬁes two\\n\\nvideo ﬁles to be the same whether they are compressed\\n\\nor not. Miners time to mine a block requires the miner to\\n\\ncomplete the compression process (by going through each\\n\\nsub-compression steps), meet the quality measure require-\\n\\nment, compute the merkle hash tree of a GOP, chain the\\n\\nvideo frames, encrypt the content, ﬁnd a storage node (or his\\n\\nown local resources) which ensures storage space required\\n\\nto store the compressed content (generate a proof). In order\\n\\nto make the size of a block even smaller, we employ Merkle\\n\\nhash tree of GOPs as well. The way the Merkle root is\\n\\ncomputed is shown in Figure 2.\\n\\nOnce this work is done, the mining node broadcasts\\n\\nthe block/s to the network that has all the information\\n\\nabout the GOP except the bulk data itself. All veriﬁer nodes\\n\\nwhich receive this block begin the veriﬁcation process which\\n\\nwould involve:\\n\\na. A comprehensive check whether GOPs are really\\n\\nstored in the designated locations. This requires\\n\\npreparing intelligent challenges for provers (miners)\\n\\nfor PoS.\\n\\nb. A comprehensive check for the merkle hash by re-\\n\\nquesting hashes of the frames and GOPs from stor-\\n\\nage node/s.\\n\\nc. A comprehensive check for the quality measure\\n\\nwhether it meets or not, using the uncompressed\\n\\nGOP data. This is to ensure that the miner nodes\\n\\nare legitimately compressed the multimedia ﬁle. This\\n\\neffort in fact characterizes a form of Proof of Com-\\n\\npression (PoC).\\n\\nOnce veriﬁed, all uncompressed copies are removed\\n\\nfrom veriﬁer node caches to open storage space for the\\n\\nnext uncompressed GOP/s data. As can be observed, since\\n\\nblocks do not include transactions in our case, we keep\\n\\nthe size of the block that contains metadata for GOP/s to\\n\\naround only a fraction of KBs. This is to limit the total\\n\\nsize of the blockchain stored in all of the veriﬁer nodes.\\n\\nOne can realize that as we include more metadata (sensor\\n\\n6\\n\\nFig. 3. Data chaining combined with video compression and predictive frame coding. .\\n\\ninformation etc.) in the blockchain, we can increase the secu-\\n\\nrity, authenticity and reliability of the system at the expense\\n\\nof reduced scalability and throughput, which is infact the\\n\\nfundamental trade-off any blockchain system faces today.\\n\\nDepending on the compression scheme, video frames\\n\\ncan be predicted from each other. In a typical compression\\n\\nscenario, we can classify frames as I and P frames where\\n\\nI is intra-coded frame i.e., the image gets compressed by\\n\\nitself whereas P frames are predicted from the associated I\\n\\nframe and one previous P frame. In another compression\\n\\nscenario, we can have B frames that shall be predicted from\\n\\ntwo or more P frames. A GOP will contain one I frame\\n\\nin the beginning and all the rest would be P (and/or B)\\n\\nframes. In order to chain the data for immutability, I frames\\n\\nof a video source ﬁle are selected to contain a hash value of\\n\\nthe previous I frame and the latest P frame in the previous\\n\\nGOP. Due to predictive nature of compression algorithm, P\\n\\nframes are automatically chained to I frames and hence are\\n\\nnot separately' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' chained using cryptographic functions. This\\n\\nwill reduce the computation requirements due to hashing\\n\\nprocess as the increased size of bulk data leads to more\\n\\ncomputation to complete the hashing operation. A detailed\\n\\nillustration of how the prediction and the hashing are done\\n\\nall together in the compress-store architecture is brieﬂy\\n\\nshown in Figure 3.\\n\\nNext we provide the summary of steps for a full node to\\n\\ninitiate, mine, store and verify a recorded video.\\n\\n1) A video ﬁle is streamed to the miner network nodes\\n\\nGOP by GOP (GOPs can be thought as transactions\\n\\nin crypto context and these are referred as GOP\\n\\ntransactions in our context) where each GOP trans-\\n\\nactions (txns) is digitally signed by the issuer for\\n\\nauthentication. This step is used to prevent potential\\n\\noutsourcing and authenticate the work (both for\\n\\nPoW and PoS). The speciﬁc format of GOP trans-\\n\\nactions are implementation-speciﬁc.\\n\\n2) Miner nodes collect/pack a set of GOP txns, au-\\n\\nthenticate them, process them and then compose\\n\\na (associated) set of subblocks to make up a block\\n\\n(block size determines the number of subblocks that\\n\\ncan be bundled together) that also contains the hash\\n\\nof the previous block.\\n\\nIn one application of proposed idea, miners PoW\\n\\nmay include compression and encryption of the\\n\\n3)\\n\\nset of collected/packed GOP data (txns). This is\\n\\nreferred as processing in step 3.\\n\\n4) On the other hand, miners PoS include a proof for\\n\\nthe storage of the compressed and encrypted con-\\n\\ntent included with the prepared block. The storage\\n\\ncan be provided with cloud services or any other\\n\\npeer of the network with local persistent storage.\\n\\nHowever, with digital signature requirement, out-\\n\\nsourcing may be forbidden by the internal system\\n\\nmanagement.\\n\\n5) Miners place the location of the compressed data,\\n\\nassociated hash values such as merkle tree root or\\n\\nleaf node hashes, compression algorithm name and\\n\\nparameters, quality measure, any additional data\\n\\n(such as proofs) that would be useful for veriﬁcation\\n\\ninto the (subblocks) blocks and broadcast it for\\n\\nveriﬁcation.\\n\\n6) Veriﬁer nodes read the contents of the block and eas-\\n\\nily verify that the content is accurately compressed,\\n\\nproperly encrypted, and stored according to a pre-\\n\\ndeﬁned quality measure (using various proofs in-\\n\\ncluded with blocks).\\n\\n7) Once the veriﬁcation process successfully ends, the\\n\\nblock is added to the local blockchain. If any of the\\n\\nrequirements is not satisﬁed, the block is not added\\n\\nto the blockchain and the veriﬁer node moves on to\\n\\nthe next veriﬁcation process waiting in the network.\\n\\nSome potential problems and workarounds: One of\\n\\nthe risks is the following. Particularly in public domain,\\n\\nbroadcasting the whole raw video GOP by GOP might be\\n\\ntoo bandwidth consuming and will lead to data trafﬁc most\\n\\nof which is useless (except for the node that successfully\\n\\nfulﬁll all the requirements of mining). As a solution, initiator\\n\\nnodes may store their raw videos either locally or remotely\\n\\nand broadcasts the location and hash value of data for\\n\\nminers to download and check. This will make download\\n\\nspeed and bandwidth be part of the equation in PoWS. In\\n\\ncase of sharding, miners may only download GOPs that are\\n\\nnot mined yet which will lead to efﬁcient use of network\\n\\nresources.\\n\\n3.2 Advantages and Differentiation compared to the\\n\\nstate-of-the-art\\n\\nFirst of all, the proposed scheme ensures data immutability\\n\\nthrough the blockchain as well as data chaining that con-\\n\\n6\\n\\nFig. 3. Data chaining combined with video compression and predictive frame coding. .\\n\\ninformation etc.) in the blockchain, we can increase the secu-\\n\\nrity, authenticity and reliability of the system at the expense\\n\\nof reduced scalability and throughput, which is infact the\\n\\nfundamental trade-off any blockchain system faces today.\\n\\nDepending on the compression scheme, video frames\\n\\n' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='can be predicted from each other. In a typical compression\\n\\nscenario, we can classify frames as I and P frames where\\n\\nI is intra-coded frame i.e., the image gets compressed by\\n\\nitself whereas P frames are predicted from the associated I\\n\\nframe and one previous P frame. In another compression\\n\\nscenario, we can have B frames that shall be predicted from\\n\\ntwo or more P frames. A GOP will contain one I frame\\n\\nin the beginning and all the rest would be P (and/or B)\\n\\nframes. In order to chain the data for immutability, I frames\\n\\nof a video source ﬁle are selected to contain a hash value of\\n\\nthe previous I frame and the latest P frame in the previous\\n\\nGOP. Due to predictive nature of compression algorithm, P\\n\\nframes are automatically chained to I frames and hence are\\n\\nnot separately chained using cryptographic functions. This\\n\\nwill reduce the computation requirements due to hashing\\n\\nprocess as the increased size of bulk data leads to more\\n\\ncomputation to complete the hashing operation. A detailed\\n\\nillustration of how the prediction and the hashing are done\\n\\nall together in the compress-store architecture is brieﬂy\\n\\nshown in Figure 3.\\n\\nNext we provide the summary of steps for a full node to\\n\\ninitiate, mine, store and verify a recorded video.\\n\\n1) A video ﬁle is streamed to the miner network nodes\\n\\nGOP by GOP (GOPs can be thought as transactions\\n\\nin crypto context and these are referred as GOP\\n\\ntransactions in our context) where each GOP trans-\\n\\nactions (txns) is digitally signed by the issuer for\\n\\nauthentication. This step is used to prevent potential\\n\\noutsourcing and authenticate the work (both for\\n\\nPoW and PoS). The speciﬁc format of GOP trans-\\n\\nactions are implementation-speciﬁc.\\n\\n2) Miner nodes collect/pack a set of GOP txns, au-\\n\\nthenticate them, process them and then compose\\n\\na (associated) set of subblocks to make up a block\\n\\n(block size determines the number of subblocks that\\n\\ncan be bundled together) that also contains the hash\\n\\nof the previous block.\\n\\nIn one application of proposed idea, miners PoW\\n\\nmay include compression and encryption of the\\n\\n3)\\n\\nset of collected/packed GOP data (txns). This is\\n\\nreferred as processing in step 3.\\n\\n4) On the other hand, miners PoS include a proof for\\n\\nthe storage of the compressed and encrypted con-\\n\\ntent included with the prepared block. The storage\\n\\ncan be provided with cloud services or any other\\n\\npeer of the network with local persistent storage.\\n\\nHowever, with digital signature requirement, out-\\n\\nsourcing may be forbidden by the internal system\\n\\nmanagement.\\n\\n5) Miners place the location of the compressed data,\\n\\nassociated hash values such as merkle tree root or\\n\\nleaf node hashes, compression algorithm name and\\n\\nparameters, quality measure, any additional data\\n\\n(such as proofs) that would be useful for veriﬁcation\\n\\ninto the (subblocks) blocks and broadcast it for\\n\\nveriﬁcation.\\n\\n6) Veriﬁer nodes read the contents of the block and eas-\\n\\nily verify that the content is accurately compressed,\\n\\nproperly encrypted, and stored according to a pre-\\n\\ndeﬁned quality measure (using various proofs in-\\n\\ncluded with blocks).\\n\\n7) Once the veriﬁcation process successfully ends, the\\n\\nblock is added to the local blockchain. If any of the\\n\\nrequirements is not satisﬁed, the block is not added\\n\\nto the blockchain and the veriﬁer node moves on to\\n\\nthe next veriﬁcation process waiting in the network.\\n\\nSome potential problems and workarounds: One of\\n\\nthe risks is the following. Particularly in public domain,\\n\\nbroadcasting the whole raw video GOP by GOP might be\\n\\ntoo bandwidth consuming and will lead to data trafﬁc most\\n\\nof which is useless (except for the node that successfully\\n\\nfulﬁll all the requirements of mining). As a solution, initiator\\n\\nnodes may store their raw videos either locally or remotely\\n\\nand broadcasts the location and hash value of data for\\n\\nminers to download and check. This will make' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' download\\n\\nspeed and bandwidth be part of the equation in PoWS. In\\n\\ncase of sharding, miners may only download GOPs that are\\n\\nnot mined yet which will lead to efﬁcient use of network\\n\\nresources.\\n\\n3.2 Advantages and Differentiation compared to the\\n\\nstate-of-the-art\\n\\nFirst of all, the proposed scheme ensures data immutability\\n\\nthrough the blockchain as well as data chaining that con-\\n\\nnects I frames (self-compressed frames) of the compressed\\n\\nvideo as described. The predictive nature of the compression\\n\\nprocess is used to add an extra layer of chaining between\\n\\ndifferent kinds of compressed video frames. Thus, any\\n\\ntempering on data can immediately be detected since this\\n\\nattempt will change all hash results in a propagated fashion.\\n\\nPlus, changing a block content will require all following\\n\\nblocks to change which will require PoWS for all GOPs cov-\\n\\nered by these blocks. This would require a large volume of\\n\\ncomputation as well as secured storage space. Secondly, the\\n\\nconcept of PoWS does not allow our system to have a solely\\n\\nCPU-based mining which can lead to hardware speciﬁc\\n\\nimplementations and hence centralization. Additionally un-\\n\\nlike bitcoins proof of work mechanism, PoW component of\\n\\nPoWS does require miners to perform useful work, i.e., in\\n\\nan application of the proposed idea, a miner can compress\\n\\na video ﬁle using his CPU and network resources. This\\n\\nway, miner will make his job easier when ﬁnding nodes to\\n\\nstore the compressed content. More compression will help\\n\\nminers spend less for data storage and bandwidth at the\\n\\nexpense of more CPU power. On the other end, Miners can\\n\\nchoose to go with a simpler compression technique at the ex-\\n\\npense of larger storage space committed for the compressed\\n\\ncontent. Such variations of the proposed idea lead miners\\n\\nto complete the total work of PoWS at different instants\\n\\nof time and hence block generation happens at relatively\\n\\ndifferent times. As a result of that, potential (soft) forks\\n\\n(in the blockchain convergence process) will be eliminated\\n\\nwithout them getting too large (lengthy) i.e., convergence of\\n\\nconsensus will be faster.\\n\\n4 CONCLUSION AND FUTURE WORK\\n\\nIn this study, a genuine video surveillance system based on\\n\\ndigital ledger technology is presented. the proposed scheme\\n\\nuses data compression and storage as means of proofs in\\n\\norder to provide green consensus which would help net-\\n\\nwork participants to commit their resources for useful work.\\n\\nFinally, the details of the proposed scheme is presented\\n\\nby providing comparisons to state-of-the-art schemes. As\\n\\nfuture work, we would like to extend the implementation\\n\\ndetails of the system to include image ﬁles. The application\\n\\nareas can be extended to include image restoration, mul-\\n\\ntimedia regeneration and data mining. Finally, large-scale\\n\\nsimulations are envisioned to demonstrate the effectiveness\\n\\nof the proposed idea in real test beds.\\n\\nREFERENCES\\n\\n[1] Nordrum, A. (2016). The internet of fewer things [news]. IEEE\\n\\nSpectrum, 53(10), 12-13.\\n\\n[2] Palankar, M. R., Iamnitchi, A., Ripeanu, M., & Garﬁnkel, S. (2008,\\n\\nJune). Amazon S3 for science grids: a viable solution?. In Proceed-\\n\\nings of the 2008 international workshop on Data-aware distributed\\n\\ncomputing (pp. 55-64). ACM.\\n\\n[3] Pilkington, M. (2016). 11 Blockchain technology: principles and\\n\\napplications. Research handbook on digital transformations, 225.\\n\\n[4] Lamport, L. (2001). Paxos made simple. ACM Sigact News, 32(4),\\n\\n18-25.\\n\\n[5] Ongaro, D., and Ousterhout, J. (2014). In search of an under-\\n\\nstandable consensus algorithm. In 2014 USENIX Annual Technical\\n\\nConference (USENIXATC 14) (pp. 305-319).\\n\\n[6] Baliga, A. (2017). Understanding blockchain consensus models. In\\n\\nPersistent.\\n\\n[7] Nakamoto, S. (2008). Bitcoin: A peer-to-peer electronic cash' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' system.\\n\\n7\\n\\n[8] Wood, G. (2014). Ethereum: A secure decentralised generalised\\n\\ntransaction ledger. Ethereum project yellow paper, 151, 1-32.\\n\\n[9] Karaﬁloski, E., and Mishev, A. (2017, July). Blockchain solutions for\\n\\nbig data challenges: A literature review. In IEEE EUROCON 2017-\\n\\n17th International Conference on Smart Technologies (pp. 763-768).\\n\\nIEEE.\\n\\n[10] Pouwelse, J., Garbacki, P., Epema, D., and Sips, H. (2005, Febru-\\n\\nary). The bittorrent p2p ﬁle-sharing system: Measurements and\\n\\nanalysis. In International Workshop on Peer-to-Peer Systems (pp.\\n\\n205-216). Springer, Berlin, Heidelberg.\\n\\n[11] Benet, J. (2014). Ipfs-content addressed, versioned, p2p ﬁle system.\\n\\narXiv preprint arXiv:1407.3561.\\n\\n[12] Drago, I., Mellia, M., M Munafo, M., Sperotto, A., Sadre, R., &\\n\\nPras, A. (2012, November). Inside dropbox: understanding personal\\n\\ncloud storage services. In Proceedings of the 2012 Internet Measure-\\n\\nment Conference (pp. 481-494). ACM.\\n\\n[13] Vorick, D., & Champine, L. (2014). Sia: Simple decentralized stor-\\n\\nage. White paper available at https://sia. tech/sia. pdf.\\n\\n[14] Wilkinson, S., Boshevski, T., Brandoff, J., and Buterin, V. (2014).\\n\\nStorj a peer-to-peer cloud storage network.\\n\\n[15] J. H. Hartman, I. Murdock, and T. Spalink, The swarm scalable\\n\\nstorage system, in Distributed Computing Systems, 1999. Proceed-\\n\\nings. 19th IEEE International Conference on. IEEE, 1999, pp. 7481.\\n\\n[16] Techical Report. Filecoin: A Cryptocurrency Operated File Net-\\n\\nwork. http: //ﬁlecoin.io/ﬁlecoin.pdf, 2014.\\n\\n[17] Paul, G., Hutchison, F., & Irvine, J. (2014, May). Security of\\n\\nthe MaidSafe vault network. In Wireless World Research Forum\\n\\nMeeting 32 (WWRF32).\\n\\n[18] Shafagh, H., Burkhalter, L., Hithnawi, A., and Duquennoy, S.\\n\\n(2017, November). Towards blockchain-based auditable storage and\\n\\nsharing of iot data. In Proceedings of the 2017 on Cloud Computing\\n\\nSecurity Workshop (pp. 45-50). ACM.\\n\\n[19] McConaghy, T., Marques, R., Mller, A., De Jonghe, D., McConaghy,\\n\\nT., McMullen, G., & Granzotto, A. (2016). BigchainDB: a scalable\\n\\nblockchain database. white paper, BigChainDB.\\n\\n[20] Dillet, R. (2015). Stampery Now Lets You Certify Documents\\n\\nUsing the Blockchain and Your Real Identity. Nov, 20, 6.\\n\\n[21] Verif-y: Blockchain Solution For Sustainable Self-Sovereign Iden-\\n\\n[22] Teutsch,\\n\\ntity. Available online; https://verif-y.com/.\\n\\nJ., & Reitwiener, C.\\n\\n(2017). A scalable veriﬁcation\\n\\nsolution for blockchains. URL: https://people. cs. uchicago.\\n\\nedu/teutsch/papers/truebit pdf.\\n\\n[23] Liu, M., Teng, Y., Leung, V. C., & Song, M. A Novel Resource\\n\\nManagement Scheme for Blockchain-based Video Streaming with\\n\\nMobile Edge Computing.\\n\\n[24] COintube: Decentralized Video Platform. Available online:\\n\\nhttps://cointube.org/.\\n\\n[25] Liang, X., Shetty, S., Tosh, D., Kamhoua, C., Kwiat, K., & Njilla,\\n\\nL. (2017, May). Provchain: A blockchain-based data provenance' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content='\\n\\narchitecture in cloud environment with enhanced privacy and\\n\\navailability. In Proceedings of the 17th IEEE/ACM international\\n\\nsymposium on cluster, cloud and grid computing (pp. 468-477).\\n\\nIEEE Press.\\n\\n[26] NEM\\n\\nTechnical\\n\\nReference,\\n\\nonline:\\n\\nAvailable\\n\\n[Online].\\n\\ncontent/themes/nem/ﬁles/NEM tech.\\n\\nVersion\\n\\n1.2.\\n\\n2018.\\n\\nhttps://nem.io/wp-\\n\\n[27] Calder, B., Wang, J., Ogus, A., Nilakantan, N., Skjolsvold, A., McK-\\n\\nelvie, S., & Haridas, J. (2011, October). Windows Azure Storage: a\\n\\nhighly available cloud storage service with strong consistency. In\\n\\nProceedings of the Twenty-Third ACM Symposium on Operating\\n\\nSystems Principles (pp. 143-157). ACM.\\n\\n[28] Vukoli, M. (2015, October). The quest for scalable blockchain fab-\\n\\nric: Proof-of-work vs. BFT replication. In International workshop on\\n\\nopen problems in network security (pp. 112-125). Springer, Cham.\\n\\n[29] Sankar, L. S., Sindhu, M., & Sethumadhavan, M. (2017, January).\\n\\nSurvey of consensus protocols on blockchain applications. In 2017\\n\\n4th International Conference on Advanced Computing and Com-\\n\\nmunication Systems (ICACCS) (pp. 1-5). IEEE.\\n\\n[30] Ateniese, G., Burns, R., Curtmola, R., Herring, J., Kissner, L.,\\n\\nPeterson, Z., & Song, D. (2007, October). Provable data possession\\n\\nat untrusted stores. In Proceedings of the 14th ACM conference on\\n\\nComputer and communications security (pp. 598-609). ACM.\\n\\n[31] Luu, L., Narayanan, V., Zheng, C., Baweja, K., Gilbert, S., & Saxena,\\n\\nP. (2016, October). A secure sharding protocol for open blockchains.\\n\\nIn Proceedings of the 2016 ACM SIGSAC Conference on Computer\\n\\nand Communications Security (pp. 17-30). ACM.\\n\\nnects I frames (self-compressed frames) of the compressed\\n\\nvideo as described. The predictive nature of the compression\\n\\nprocess is used to add an extra layer of chaining between\\n\\ndifferent kinds of compressed video frames. Thus, any\\n\\ntempering on data can immediately be detected since this\\n\\nattempt will change all hash results in a propagated fashion.\\n\\nPlus, changing a block content will require all following\\n\\nblocks to change which will require PoWS for all GOPs cov-\\n\\nered by these blocks. This would require a large volume of\\n\\ncomputation as well as secured storage space. Secondly, the\\n\\nconcept of PoWS does not allow our system to have a solely\\n\\nCPU-based mining which can lead to hardware speciﬁc\\n\\nimplementations and hence centralization. Additionally un-\\n\\nlike bitcoins proof of work mechanism, PoW component of\\n\\nPoWS does require miners to perform useful work, i.e., in\\n\\nan application of the proposed idea, a miner can compress\\n\\na video ﬁle using his CPU and network resources. This\\n\\nway, miner will make his job easier when ﬁnding nodes to\\n\\nstore the compressed content. More compression will help\\n\\nminers spend less for data storage and bandwidth at the\\n\\nexpense of more CPU power. On the other end, Miners can\\n\\nchoose to go with a simpler compression technique at the ex-\\n\\npense of larger storage space committed for the compressed\\n\\ncontent. Such variations of the proposed idea lead miners\\n\\nto complete the total work of PoWS at different instants\\n\\nof time and hence block generation happens at relatively\\n\\ndifferent times. As a result of that, potential (soft) forks\\n\\n(in the blockchain convergence process) will be eliminated\\n\\nwithout them getting too large (lengthy) i.e., convergence of\\n\\nconsensus will be faster.\\n\\n4 CONCLUSION AND FUTURE WORK\\n\\nIn this study, a genuine video surveillance system based on\\n\\ndigital ledger technology is presented. the proposed scheme\\n\\nuses data compression and storage as means of proofs in\\n\\norder to provide' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' green consensus which would help net-\\n\\nwork participants to commit their resources for useful work.\\n\\nFinally, the details of the proposed scheme is presented\\n\\nby providing comparisons to state-of-the-art schemes. As\\n\\nfuture work, we would like to extend the implementation\\n\\ndetails of the system to include image ﬁles. The application\\n\\nareas can be extended to include image restoration, mul-\\n\\ntimedia regeneration and data mining. Finally, large-scale\\n\\nsimulations are envisioned to demonstrate the effectiveness\\n\\nof the proposed idea in real test beds.\\n\\nREFERENCES\\n\\n[1] Nordrum, A. (2016). The internet of fewer things [news]. IEEE\\n\\nSpectrum, 53(10), 12-13.\\n\\n[2] Palankar, M. R., Iamnitchi, A., Ripeanu, M., & Garﬁnkel, S. (2008,\\n\\nJune). Amazon S3 for science grids: a viable solution?. In Proceed-\\n\\nings of the 2008 international workshop on Data-aware distributed\\n\\ncomputing (pp. 55-64). ACM.\\n\\n[3] Pilkington, M. (2016). 11 Blockchain technology: principles and\\n\\napplications. Research handbook on digital transformations, 225.\\n\\n[4] Lamport, L. (2001). Paxos made simple. ACM Sigact News, 32(4),\\n\\n18-25.\\n\\n[5] Ongaro, D., and Ousterhout, J. (2014). In search of an under-\\n\\nstandable consensus algorithm. In 2014 USENIX Annual Technical\\n\\nConference (USENIXATC 14) (pp. 305-319).\\n\\n[6] Baliga, A. (2017). Understanding blockchain consensus models. In\\n\\nPersistent.\\n\\n[7] Nakamoto, S. (2008). Bitcoin: A peer-to-peer electronic cash system.\\n\\n7\\n\\n[8] Wood, G. (2014). Ethereum: A secure decentralised generalised\\n\\ntransaction ledger. Ethereum project yellow paper, 151, 1-32.\\n\\n[9] Karaﬁloski, E., and Mishev, A. (2017, July). Blockchain solutions for\\n\\nbig data challenges: A literature review. In IEEE EUROCON 2017-\\n\\n17th International Conference on Smart Technologies (pp. 763-768).\\n\\nIEEE.\\n\\n[10] Pouwelse, J., Garbacki, P., Epema, D., and Sips, H. (2005, Febru-\\n\\nary). The bittorrent p2p ﬁle-sharing system: Measurements and\\n\\nanalysis. In International Workshop on Peer-to-Peer Systems (pp.\\n\\n205-216). Springer, Berlin, Heidelberg.\\n\\n[11] Benet, J. (2014). Ipfs-content addressed, versioned, p2p ﬁle system.\\n\\narXiv preprint arXiv:1407.3561.\\n\\n[12] Drago, I., Mellia, M., M Munafo, M., Sperotto, A., Sadre, R., &\\n\\nPras, A. (2012, November). Inside dropbox: understanding personal\\n\\ncloud storage services. In Proceedings of the 2012 Internet Measure-\\n\\nment Conference (pp. 481-494). ACM.\\n\\n[13] Vorick, D., & Champine, L. (2014). Sia: Simple decentralized stor-\\n\\nage. White paper available at https://sia. tech/sia. pdf.\\n\\n[14] Wilkinson, S., Boshevski, T., Brandoff, J., and Buterin, V. (2014).\\n\\nStorj a peer-to-peer cloud storage network.\\n\\n[15] J. H. Hartman, I. Murdock, and T. Spalink, The swarm scalable\\n\\nstorage system, in Distributed Computing Systems, 1999. Proceed-\\n\\nings. 19th IEEE International Conference on. IEEE, 1999, pp. 7481.\\n\\n[16] Techical Report. Filecoin: A Cryptocurrency Operated File Net-\\n\\nwork. http: //ﬁlecoin.io/ﬁlecoin.pdf, 2014.\\n\\n[17] Paul, G., Hutchison, F., & Irvine, J. (2014, May). Security of\\n\\nthe MaidSafe vault network. In Wireless World Research Forum\\n\\nMeeting' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n",
      "page_content=' 32 (WWRF32).\\n\\n[18] Shafagh, H., Burkhalter, L., Hithnawi, A., and Duquennoy, S.\\n\\n(2017, November). Towards blockchain-based auditable storage and\\n\\nsharing of iot data. In Proceedings of the 2017 on Cloud Computing\\n\\nSecurity Workshop (pp. 45-50). ACM.\\n\\n[19] McConaghy, T., Marques, R., Mller, A., De Jonghe, D., McConaghy,\\n\\nT., McMullen, G., & Granzotto, A. (2016). BigchainDB: a scalable\\n\\nblockchain database. white paper, BigChainDB.\\n\\n[20] Dillet, R. (2015). Stampery Now Lets You Certify Documents\\n\\nUsing the Blockchain and Your Real Identity. Nov, 20, 6.\\n\\n[21] Verif-y: Blockchain Solution For Sustainable Self-Sovereign Iden-\\n\\n[22] Teutsch,\\n\\ntity. Available online; https://verif-y.com/.\\n\\nJ., & Reitwiener, C.\\n\\n(2017). A scalable veriﬁcation\\n\\nsolution for blockchains. URL: https://people. cs. uchicago.\\n\\nedu/teutsch/papers/truebit pdf.\\n\\n[23] Liu, M., Teng, Y., Leung, V. C., & Song, M. A Novel Resource\\n\\nManagement Scheme for Blockchain-based Video Streaming with\\n\\nMobile Edge Computing.\\n\\n[24] COintube: Decentralized Video Platform. Available online:\\n\\nhttps://cointube.org/.\\n\\n[25] Liang, X., Shetty, S., Tosh, D., Kamhoua, C., Kwiat, K., & Njilla,\\n\\nL. (2017, May). Provchain: A blockchain-based data provenance\\n\\narchitecture in cloud environment with enhanced privacy and\\n\\navailability. In Proceedings of the 17th IEEE/ACM international\\n\\nsymposium on cluster, cloud and grid computing (pp. 468-477).\\n\\nIEEE Press.\\n\\n[26] NEM\\n\\nTechnical\\n\\nReference,\\n\\nonline:\\n\\nAvailable\\n\\n[Online].\\n\\ncontent/themes/nem/ﬁles/NEM tech.\\n\\nVersion\\n\\n1.2.\\n\\n2018.\\n\\nhttps://nem.io/wp-\\n\\n[27] Calder, B., Wang, J., Ogus, A., Nilakantan, N., Skjolsvold, A., McK-\\n\\nelvie, S., & Haridas, J. (2011, October). Windows Azure Storage: a\\n\\nhighly available cloud storage service with strong consistency. In\\n\\nProceedings of the Twenty-Third ACM Symposium on Operating\\n\\nSystems Principles (pp. 143-157). ACM.\\n\\n[28] Vukoli, M. (2015, October). The quest for scalable blockchain fab-\\n\\nric: Proof-of-work vs. BFT replication. In International workshop on\\n\\nopen problems in network security (pp. 112-125). Springer, Cham.\\n\\n[29] Sankar, L. S., Sindhu, M., & Sethumadhavan, M. (2017, January).\\n\\nSurvey of consensus protocols on blockchain applications. In 2017\\n\\n4th International Conference on Advanced Computing and Com-\\n\\nmunication Systems (ICACCS) (pp. 1-5). IEEE.\\n\\n[30] Ateniese, G., Burns, R., Curtmola, R., Herring, J., Kissner, L.,\\n\\nPeterson, Z., & Song, D. (2007, October). Provable data possession\\n\\nat untrusted stores. In Proceedings of the 14th ACM conference on\\n\\nComputer and communications security (pp. 598-609). ACM.\\n\\n[31] Luu, L., Narayanan, V., Zheng, C., Baweja, K., Gilbert, S., & Saxena,\\n\\nP. (2016, October). A secure sharding protocol for open blockchains.\\n\\nIn Proceedings of the 2016 ACM SIGSAC Conference on Computer\\n\\nand Communications Security (pp. 17-30). ACM.' lookup_str='' metadata={'source': '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'} lookup_index=0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "test = Chroma.from_documents(docs, embeddings)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(docs, embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "embeddings= OpenAIEmbeddings()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\n",
    "test = Chroma.from_documents(docs, embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "client = chromadb.Client()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n",
      "Exiting: Cleaning up .chroma directory\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "oak = Chroma()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "client.reset()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "\n",
    "loader = UnstructuredFileLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf', strategy='fast')\n",
    "documents = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loader \u001b[39m=\u001b[39m UnstructuredFileLoader(\u001b[39m'\u001b[39m\u001b[39m/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf\u001b[39m\u001b[39m'\u001b[39m, strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfast\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:61\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=59'>60</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=60'>61</a>\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=62'>63</a>\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:95\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=92'>93</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py:45\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, file, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=18'>19</a>\u001b[0m     filename: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=19'>20</a>\u001b[0m     file: Optional[IO] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=22'>23</a>\u001b[0m     encoding: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=23'>24</a>\u001b[0m ):\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=24'>25</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Partitions a document into its constituent elements. Will use libmagic to determine\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=25'>26</a>\u001b[0m \u001b[39m    the file's type and route it to the appropriate partitioning function. Applies the default\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=26'>27</a>\u001b[0m \u001b[39m    parameters for each partitioning function. Use the document-type specific partitioning\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=42'>43</a>\u001b[0m \u001b[39m        The encoding method used to decode the text input. If None, utf-8 will be used.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=43'>44</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=44'>45</a>\u001b[0m     filetype \u001b[39m=\u001b[39m detect_filetype(filename\u001b[39m=\u001b[39;49mfilename, file\u001b[39m=\u001b[39;49mfile)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=47'>48</a>\u001b[0m         file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py:149\u001b[0m, in \u001b[0;36mdetect_filetype\u001b[0;34m(filename, file)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=146'>147</a>\u001b[0m extension \u001b[39m=\u001b[39m extension\u001b[39m.\u001b[39mlower()\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=147'>148</a>\u001b[0m \u001b[39mif\u001b[39;00m LIBMAGIC_AVAILABLE:\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=148'>149</a>\u001b[0m     mime_type \u001b[39m=\u001b[39m magic\u001b[39m.\u001b[39;49mfrom_file(filename, mime\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=149'>150</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=150'>151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m EXT_TO_FILETYPE\u001b[39m.\u001b[39mget(extension\u001b[39m.\u001b[39mlower(), FileType\u001b[39m.\u001b[39mUNK)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py:179\u001b[0m, in \u001b[0;36mfrom_file\u001b[0;34m(filename, mime)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=170'>171</a>\u001b[0m \u001b[39mAccepts a filename and returns the detected filetype.  Return\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=171'>172</a>\u001b[0m \u001b[39mvalue is the mimetype if mime=True, otherwise a human readable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=175'>176</a>\u001b[0m \u001b[39m'application/pdf'\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=176'>177</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=177'>178</a>\u001b[0m m \u001b[39m=\u001b[39m _get_magic_type(mime)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=178'>179</a>\u001b[0m \u001b[39mreturn\u001b[39;00m m\u001b[39m.\u001b[39;49mfrom_file(filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py:112\u001b[0m, in \u001b[0;36mMagic.from_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=109'>110</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_file\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=110'>111</a>\u001b[0m     \u001b[39m# raise FileNotFoundException or IOError if the file does not exist\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=111'>112</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _real_open(filename):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=112'>113</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/magic/__init__.py?line=114'>115</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "\n",
    "loader = UnstructuredFileLoader('/home/gyasis/Downloads/1810.05934.pdf', strategy='fast')\n",
    "documents = loader.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "embeddings= OpenAIEmbeddings()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "vectordb = Chroma.from_documents(docs, embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "#test loading database...you can point to directory with persist_directory =\n",
    "test = Chroma(embedding_function = embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "dir(vector)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdir\u001b[39m(vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "dir(vectordb)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_client',\n",
       " '_client_settings',\n",
       " '_collection',\n",
       " '_embedding_function',\n",
       " '_persist_directory',\n",
       " 'add_documents',\n",
       " 'add_texts',\n",
       " 'delete_collection',\n",
       " 'from_documents',\n",
       " 'from_texts',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'persist',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_with_score']"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "dir(langchain.chains)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'langchain' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdir\u001b[39m(langchain\u001b[39m.\u001b[39mchains)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'langchain' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import langchain"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "dir(langchain.chains)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['APIChain',\n",
       " 'AnalyzeDocumentChain',\n",
       " 'ChatVectorDBChain',\n",
       " 'ConstitutionalChain',\n",
       " 'ConversationChain',\n",
       " 'GraphQAChain',\n",
       " 'HypotheticalDocumentEmbedder',\n",
       " 'LLMBashChain',\n",
       " 'LLMChain',\n",
       " 'LLMCheckerChain',\n",
       " 'LLMMathChain',\n",
       " 'LLMRequestsChain',\n",
       " 'LLMSummarizationCheckerChain',\n",
       " 'MapReduceChain',\n",
       " 'OpenAIModerationChain',\n",
       " 'PALChain',\n",
       " 'QAGenerationChain',\n",
       " 'QAWithSourcesChain',\n",
       " 'SQLDatabaseChain',\n",
       " 'SQLDatabaseSequentialChain',\n",
       " 'SequentialChain',\n",
       " 'SimpleSequentialChain',\n",
       " 'TransformChain',\n",
       " 'VectorDBQA',\n",
       " 'VectorDBQAWithSourcesChain',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'base',\n",
       " 'chat_vector_db',\n",
       " 'combine_documents',\n",
       " 'constitutional_ai',\n",
       " 'conversation',\n",
       " 'graph_qa',\n",
       " 'hyde',\n",
       " 'llm',\n",
       " 'llm_bash',\n",
       " 'llm_checker',\n",
       " 'llm_math',\n",
       " 'llm_requests',\n",
       " 'llm_summarization_checker',\n",
       " 'load_chain',\n",
       " 'loading',\n",
       " 'mapreduce',\n",
       " 'moderation',\n",
       " 'pal',\n",
       " 'prompt_selector',\n",
       " 'qa_generation',\n",
       " 'qa_with_sources',\n",
       " 'question_answering',\n",
       " 'sequential',\n",
       " 'sql_database',\n",
       " 'transform',\n",
       " 'vector_db_qa']"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "import glob\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "tree = glob.glob('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "tree"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads']"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "tree = glob.glob('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "tree\n",
    ""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/']"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "tree = glob.glob('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/*.pdf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "tree"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1712.02898.pdf',\n",
       " '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/1905.10458.pdf',\n",
       " '/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/2004.02810.pdf']"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "loader = UnstructuredFileLoader(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "oak = loader.load()\n",
    ""
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oak \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:61\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=59'>60</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=60'>61</a>\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=62'>63</a>\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:95\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=92'>93</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py:45\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, file, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=18'>19</a>\u001b[0m     filename: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=19'>20</a>\u001b[0m     file: Optional[IO] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=22'>23</a>\u001b[0m     encoding: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=23'>24</a>\u001b[0m ):\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=24'>25</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Partitions a document into its constituent elements. Will use libmagic to determine\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=25'>26</a>\u001b[0m \u001b[39m    the file's type and route it to the appropriate partitioning function. Applies the default\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=26'>27</a>\u001b[0m \u001b[39m    parameters for each partitioning function. Use the document-type specific partitioning\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=42'>43</a>\u001b[0m \u001b[39m        The encoding method used to decode the text input. If None, utf-8 will be used.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=43'>44</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=44'>45</a>\u001b[0m     filetype \u001b[39m=\u001b[39m detect_filetype(filename\u001b[39m=\u001b[39;49mfilename, file\u001b[39m=\u001b[39;49mfile)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=47'>48</a>\u001b[0m         file\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py:146\u001b[0m, in \u001b[0;36mdetect_filetype\u001b[0;34m(filename, file)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly one of filename or file should be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=144'>145</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=145'>146</a>\u001b[0m     _, extension \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49msplitext(filename)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=146'>147</a>\u001b[0m     extension \u001b[39m=\u001b[39m extension\u001b[39m.\u001b[39mlower()\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/file_utils/filetype.py?line=147'>148</a>\u001b[0m     \u001b[39mif\u001b[39;00m LIBMAGIC_AVAILABLE:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/posixpath.py:118\u001b[0m, in \u001b[0;36msplitext\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/posixpath.py?line=116'>117</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplitext\u001b[39m(p):\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/posixpath.py?line=117'>118</a>\u001b[0m     p \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfspath(p)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/posixpath.py?line=118'>119</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/posixpath.py?line=119'>120</a>\u001b[0m         sep \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "loader = UnstructuredFileLoader(tree[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "oak = loader.load()\n",
    ""
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cff1a90d85940a2891796e3a0d446bc"
      },
      "text/plain": [
       "Downloading (…)\"model_final.pth\";:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e6e5e602a3543868b24e38a6ee113f3"
      },
      "text/plain": [
       "Downloading (…)50_FPN_3x/config.yml:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oak \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:61\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=59'>60</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=60'>61</a>\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=62'>63</a>\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:95\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=92'>93</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py:68\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, file, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_md(filename\u001b[39m=\u001b[39mfilename, file\u001b[39m=\u001b[39mfile, include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=66'>67</a>\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mPDF:\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_pdf(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=68'>69</a>\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=69'>70</a>\u001b[0m         file\u001b[39m=\u001b[39;49mfile,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=70'>71</a>\u001b[0m         url\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=71'>72</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=72'>73</a>\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=73'>74</a>\u001b[0m         strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=74'>75</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=75'>76</a>\u001b[0m \u001b[39melif\u001b[39;00m (filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mPNG) \u001b[39mor\u001b[39;00m (filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mJPG):\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=76'>77</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_image(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=77'>78</a>\u001b[0m         filename\u001b[39m=\u001b[39mfilename,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=78'>79</a>\u001b[0m         file\u001b[39m=\u001b[39mfile,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=79'>80</a>\u001b[0m         url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=80'>81</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=81'>82</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:51\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[0;34m(filename, file, url, template, token, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m template \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=49'>50</a>\u001b[0m     template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlayout/pdf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=51'>52</a>\u001b[0m     filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=52'>53</a>\u001b[0m     file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=53'>54</a>\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=54'>55</a>\u001b[0m     template\u001b[39m=\u001b[39;49mtemplate,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=55'>56</a>\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=56'>57</a>\u001b[0m     include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=57'>58</a>\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=58'>59</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=59'>60</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:103\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, url, template, token, is_image, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=100'>101</a>\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=101'>102</a>\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=102'>103</a>\u001b[0m         layout_elements \u001b[39m=\u001b[39m _partition_pdf_or_image_local(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=103'>104</a>\u001b[0m             filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=104'>105</a>\u001b[0m             file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=105'>106</a>\u001b[0m             template\u001b[39m=\u001b[39;49mout_template,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=106'>107</a>\u001b[0m             is_image\u001b[39m=\u001b[39;49mis_image,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=107'>108</a>\u001b[0m             include_page_breaks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=108'>109</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=110'>111</a>\u001b[0m \u001b[39melif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfast\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m fallback_to_fast:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=111'>112</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _partition_pdf_with_pdfminer(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=112'>113</a>\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=113'>114</a>\u001b[0m         file\u001b[39m=\u001b[39mfile,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=114'>115</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=115'>116</a>\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=116'>117</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:175\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_local\u001b[0;34m(filename, file, template, is_image, include_page_breaks)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=165'>166</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=166'>167</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=167'>168</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThere was a problem importing unstructured_inference module - it may not be installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=168'>169</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorrectly... try running pip install unstructured[local-inference] if you installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=169'>170</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe unstructured library as a package. If you cloned the unstructured repository, try \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=170'>171</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrunning make install-local-inference from the root directory of the repository.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=171'>172</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=173'>174</a>\u001b[0m layout \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=174'>175</a>\u001b[0m     process_file_with_model(filename, template, is_image\u001b[39m=\u001b[39;49mis_image)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=175'>176</a>\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=176'>177</a>\u001b[0m     \u001b[39melse\u001b[39;00m process_data_with_model(file, template, is_image\u001b[39m=\u001b[39mis_image)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=177'>178</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=179'>180</a>\u001b[0m \u001b[39mreturn\u001b[39;00m document_to_element_list(layout, include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py:240\u001b[0m, in \u001b[0;36mprocess_file_with_model\u001b[0;34m(filename, model_name, is_image, ocr_strategy, fixed_layouts)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=230'>231</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_file_with_model\u001b[39m(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=231'>232</a>\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=232'>233</a>\u001b[0m     model_name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=235'>236</a>\u001b[0m     fixed_layouts: Optional[List[Optional[Layout]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=236'>237</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DocumentLayout:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=237'>238</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Processes pdf file with name filename into a DocumentLayout by using a model identified by\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=238'>239</a>\u001b[0m \u001b[39m    model_name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=239'>240</a>\u001b[0m     model \u001b[39m=\u001b[39m get_model(model_name)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=240'>241</a>\u001b[0m     layout \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=241'>242</a>\u001b[0m         DocumentLayout\u001b[39m.\u001b[39mfrom_image_file(filename, model\u001b[39m=\u001b[39mmodel, ocr_strategy\u001b[39m=\u001b[39mocr_strategy)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=242'>243</a>\u001b[0m         \u001b[39mif\u001b[39;00m is_image\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=245'>246</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=246'>247</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=247'>248</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layout\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py:20\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39min\u001b[39;00m DETECTRON2_MODEL_TYPES:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=18'>19</a>\u001b[0m     model: UnstructuredModel \u001b[39m=\u001b[39m UnstructuredDetectronModel()\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=19'>20</a>\u001b[0m     model\u001b[39m.\u001b[39;49minitialize(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mDETECTRON2_MODEL_TYPES[model_name])\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=20'>21</a>\u001b[0m \u001b[39melif\u001b[39;00m model_name \u001b[39min\u001b[39;00m YOLOX_MODEL_TYPES:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=21'>22</a>\u001b[0m     model \u001b[39m=\u001b[39m UnstructuredYoloXModel()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py:84\u001b[0m, in \u001b[0;36mUnstructuredDetectronModel.initialize\u001b[0;34m(self, config_path, model_path, label_map, extra_config, device)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=81'>82</a>\u001b[0m model_path_str: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m model_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(model_path)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=82'>83</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading the Detectron2 layout model ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=83'>84</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Detectron2LayoutModel(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=84'>85</a>\u001b[0m     config_path_str,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=85'>86</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path_str,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=86'>87</a>\u001b[0m     label_map\u001b[39m=\u001b[39;49mlabel_map,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=87'>88</a>\u001b[0m     extra_config\u001b[39m=\u001b[39;49mextra_config,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=88'>89</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=89'>90</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py:119\u001b[0m, in \u001b[0;36mDetectron2LayoutModel.__init__\u001b[0;34m(self, config_path, model_path, label_map, extra_config, enforce_cpu, device)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=115'>116</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m cfg\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=117'>118</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_map \u001b[39m=\u001b[39m label_map\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=118'>119</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py:122\u001b[0m, in \u001b[0;36mDetectron2LayoutModel._create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=120'>121</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=121'>122</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m detectron2\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mDefaultPredictor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py:282\u001b[0m, in \u001b[0;36mDefaultPredictor.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=279'>280</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, cfg):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mclone()  \u001b[39m# cfg can be modified by model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=281'>282</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m build_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=282'>283</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=283'>284</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cfg\u001b[39m.\u001b[39mDATASETS\u001b[39m.\u001b[39mTEST):\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py:23\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=20'>21</a>\u001b[0m meta_arch \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mMETA_ARCHITECTURE\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m META_ARCH_REGISTRY\u001b[39m.\u001b[39mget(meta_arch)(cfg)\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mdevice(cfg\u001b[39m.\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mDEVICE))\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=23'>24</a>\u001b[0m _log_api_usage(\u001b[39m\"\u001b[39m\u001b[39mmodeling.meta_arch.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m meta_arch)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=984'>985</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=985'>986</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=986'>987</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=988'>989</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=638'>639</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=639'>640</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=640'>641</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=642'>643</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=643'>644</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=644'>645</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=645'>646</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=650'>651</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=651'>652</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=638'>639</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=639'>640</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=640'>641</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=642'>643</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=643'>644</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=644'>645</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=645'>646</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=650'>651</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=651'>652</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=659'>660</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=660'>661</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=661'>662</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=662'>663</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=663'>664</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=664'>665</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=665'>666</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=983'>984</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=984'>985</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=985'>986</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=986'>987</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "loader = UnstructuredFileLoader(tree[0],strategy='fast')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "oak = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exiting: Cleaning up .chroma directory\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "add_data_to_vectordb(oak)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'add_data_to_vectordb' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m add_data_to_vectordb(oak)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add_data_to_vectordb' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "vectordb()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'Chroma' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Chroma' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "help(vectordb.persist)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on method persist in module langchain.vectorstores.chroma:\n",
      "\n",
      "persist() -> 'None' method of langchain.vectorstores.chroma.Chroma instance\n",
      "    Persist the collection.\n",
      "    \n",
      "    This can be used to explicitly persist the data to disk.\n",
      "    It will also be called automatically when the object is destroyed.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "loader = DirectoryLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "docs = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py:61\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=58'>59</a>\u001b[0m                     logger\u001b[39m.\u001b[39mwarning(e)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=59'>60</a>\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=60'>61</a>\u001b[0m                     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=61'>62</a>\u001b[0m \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py:55\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m _is_visible(i\u001b[39m.\u001b[39mrelative_to(p)) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_hidden:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=53'>54</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=54'>55</a>\u001b[0m         sub_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_cls(\u001b[39mstr\u001b[39;49m(i))\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=55'>56</a>\u001b[0m         docs\u001b[39m.\u001b[39mextend(sub_docs)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/directory.py?line=56'>57</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:61\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=59'>60</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=60'>61</a>\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=62'>63</a>\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py:95\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=92'>93</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/document_loaders/unstructured.py?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py:68\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, file, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_md(filename\u001b[39m=\u001b[39mfilename, file\u001b[39m=\u001b[39mfile, include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=66'>67</a>\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mPDF:\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_pdf(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=68'>69</a>\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=69'>70</a>\u001b[0m         file\u001b[39m=\u001b[39;49mfile,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=70'>71</a>\u001b[0m         url\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=71'>72</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=72'>73</a>\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=73'>74</a>\u001b[0m         strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=74'>75</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=75'>76</a>\u001b[0m \u001b[39melif\u001b[39;00m (filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mPNG) \u001b[39mor\u001b[39;00m (filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mJPG):\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=76'>77</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition_image(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=77'>78</a>\u001b[0m         filename\u001b[39m=\u001b[39mfilename,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=78'>79</a>\u001b[0m         file\u001b[39m=\u001b[39mfile,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=79'>80</a>\u001b[0m         url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=80'>81</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/auto.py?line=81'>82</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:51\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[0;34m(filename, file, url, template, token, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m template \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=49'>50</a>\u001b[0m     template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlayout/pdf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=51'>52</a>\u001b[0m     filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=52'>53</a>\u001b[0m     file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=53'>54</a>\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=54'>55</a>\u001b[0m     template\u001b[39m=\u001b[39;49mtemplate,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=55'>56</a>\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=56'>57</a>\u001b[0m     include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=57'>58</a>\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=58'>59</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=59'>60</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:103\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, url, template, token, is_image, include_page_breaks, strategy, encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=100'>101</a>\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=101'>102</a>\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=102'>103</a>\u001b[0m         layout_elements \u001b[39m=\u001b[39m _partition_pdf_or_image_local(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=103'>104</a>\u001b[0m             filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=104'>105</a>\u001b[0m             file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=105'>106</a>\u001b[0m             template\u001b[39m=\u001b[39;49mout_template,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=106'>107</a>\u001b[0m             is_image\u001b[39m=\u001b[39;49mis_image,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=107'>108</a>\u001b[0m             include_page_breaks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=108'>109</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=110'>111</a>\u001b[0m \u001b[39melif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfast\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m fallback_to_fast:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=111'>112</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _partition_pdf_with_pdfminer(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=112'>113</a>\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=113'>114</a>\u001b[0m         file\u001b[39m=\u001b[39mfile,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=114'>115</a>\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=115'>116</a>\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=116'>117</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py:175\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_local\u001b[0;34m(filename, file, template, is_image, include_page_breaks)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=165'>166</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=166'>167</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=167'>168</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThere was a problem importing unstructured_inference module - it may not be installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=168'>169</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorrectly... try running pip install unstructured[local-inference] if you installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=169'>170</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe unstructured library as a package. If you cloned the unstructured repository, try \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=170'>171</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrunning make install-local-inference from the root directory of the repository.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=171'>172</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=173'>174</a>\u001b[0m layout \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=174'>175</a>\u001b[0m     process_file_with_model(filename, template, is_image\u001b[39m=\u001b[39;49mis_image)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=175'>176</a>\u001b[0m     \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=176'>177</a>\u001b[0m     \u001b[39melse\u001b[39;00m process_data_with_model(file, template, is_image\u001b[39m=\u001b[39mis_image)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=177'>178</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured/partition/pdf.py?line=179'>180</a>\u001b[0m \u001b[39mreturn\u001b[39;00m document_to_element_list(layout, include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py:240\u001b[0m, in \u001b[0;36mprocess_file_with_model\u001b[0;34m(filename, model_name, is_image, ocr_strategy, fixed_layouts)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=230'>231</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_file_with_model\u001b[39m(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=231'>232</a>\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=232'>233</a>\u001b[0m     model_name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=235'>236</a>\u001b[0m     fixed_layouts: Optional[List[Optional[Layout]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=236'>237</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DocumentLayout:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=237'>238</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Processes pdf file with name filename into a DocumentLayout by using a model identified by\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=238'>239</a>\u001b[0m \u001b[39m    model_name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=239'>240</a>\u001b[0m     model \u001b[39m=\u001b[39m get_model(model_name)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=240'>241</a>\u001b[0m     layout \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=241'>242</a>\u001b[0m         DocumentLayout\u001b[39m.\u001b[39mfrom_image_file(filename, model\u001b[39m=\u001b[39mmodel, ocr_strategy\u001b[39m=\u001b[39mocr_strategy)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=242'>243</a>\u001b[0m         \u001b[39mif\u001b[39;00m is_image\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=245'>246</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=246'>247</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/inference/layout.py?line=247'>248</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layout\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py:20\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39min\u001b[39;00m DETECTRON2_MODEL_TYPES:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=18'>19</a>\u001b[0m     model: UnstructuredModel \u001b[39m=\u001b[39m UnstructuredDetectronModel()\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=19'>20</a>\u001b[0m     model\u001b[39m.\u001b[39;49minitialize(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mDETECTRON2_MODEL_TYPES[model_name])\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=20'>21</a>\u001b[0m \u001b[39melif\u001b[39;00m model_name \u001b[39min\u001b[39;00m YOLOX_MODEL_TYPES:\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/base.py?line=21'>22</a>\u001b[0m     model \u001b[39m=\u001b[39m UnstructuredYoloXModel()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py:84\u001b[0m, in \u001b[0;36mUnstructuredDetectronModel.initialize\u001b[0;34m(self, config_path, model_path, label_map, extra_config, device)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=81'>82</a>\u001b[0m model_path_str: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m model_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(model_path)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=82'>83</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading the Detectron2 layout model ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=83'>84</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Detectron2LayoutModel(\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=84'>85</a>\u001b[0m     config_path_str,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=85'>86</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path_str,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=86'>87</a>\u001b[0m     label_map\u001b[39m=\u001b[39;49mlabel_map,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=87'>88</a>\u001b[0m     extra_config\u001b[39m=\u001b[39;49mextra_config,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=88'>89</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/unstructured_inference/models/detectron2.py?line=89'>90</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py:119\u001b[0m, in \u001b[0;36mDetectron2LayoutModel.__init__\u001b[0;34m(self, config_path, model_path, label_map, extra_config, enforce_cpu, device)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=115'>116</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m cfg\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=117'>118</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_map \u001b[39m=\u001b[39m label_map\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=118'>119</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model()\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py:122\u001b[0m, in \u001b[0;36mDetectron2LayoutModel._create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=120'>121</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/layoutparser/models/detectron2/layoutmodel.py?line=121'>122</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m detectron2\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mDefaultPredictor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py:282\u001b[0m, in \u001b[0;36mDefaultPredictor.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=279'>280</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, cfg):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mclone()  \u001b[39m# cfg can be modified by model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=281'>282</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m build_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=282'>283</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/engine/defaults.py?line=283'>284</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cfg\u001b[39m.\u001b[39mDATASETS\u001b[39m.\u001b[39mTEST):\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py:23\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=20'>21</a>\u001b[0m meta_arch \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mMETA_ARCHITECTURE\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m META_ARCH_REGISTRY\u001b[39m.\u001b[39mget(meta_arch)(cfg)\n\u001b[0;32m---> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mdevice(cfg\u001b[39m.\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mDEVICE))\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=23'>24</a>\u001b[0m _log_api_usage(\u001b[39m\"\u001b[39m\u001b[39mmodeling.meta_arch.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m meta_arch)\n\u001b[1;32m     <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/detectron2/modeling/meta_arch/build.py?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=984'>985</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=985'>986</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=986'>987</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=988'>989</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=638'>639</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=639'>640</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=640'>641</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=642'>643</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=643'>644</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=644'>645</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=645'>646</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=650'>651</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=651'>652</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=638'>639</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=639'>640</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=640'>641</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=642'>643</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=643'>644</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=644'>645</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=645'>646</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=650'>651</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=651'>652</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=659'>660</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=660'>661</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=661'>662</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=662'>663</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=663'>664</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=664'>665</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=665'>666</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=983'>984</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=984'>985</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=985'>986</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/modules/module.py?line=986'>987</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import the required libraries and modules\n",
    "import os\n",
    "import io\n",
    "from flask import Flask, render_template, request, flash, redirect\n",
    "from werkzeug.utils import secure_filename\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "import chromadb\n",
    "import pdfminer.high_level\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "import tempfile\n",
    "import shutil"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loader = DirectoryLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "docs = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 105.41it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 21.82it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 98.61it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 18.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 121.69it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 77.33it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 59.99it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 24.45it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 61.09it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 19.45it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 51.01it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 92.83it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 87.45it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 64.22it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 119.97it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 89.71it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 90.31it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 112.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72.95it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 162.46it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "docs = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 95.10it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 26.42it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 97.71it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 136.36it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 70.88it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 61.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 23.37it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 64.00it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.83it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 57.41it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 103.19it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 102.08it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 61.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 64.45it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 164.70it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 89.68it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 89.09it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 95.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 87.49it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 158.70it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import the required libraries and modules\n",
    "import os\n",
    "import io\n",
    "from flask import Flask, render_template, request, flash, redirect\n",
    "from werkzeug.utils import secure_filename\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "import chromadb\n",
    "import pdfminer.high_level\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "import tempfile\n",
    "import shutil"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loader = DirectoryLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/', strategy='fast')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'strategy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[39m=\u001b[39m DirectoryLoader(\u001b[39m'\u001b[39;49m\u001b[39m/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/\u001b[39;49m\u001b[39m'\u001b[39;49m, strategy\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfast\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'strategy'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "loader = DirectoryLoader('/media/gyasis/Blade 15 SSD/Users/gyasi/Google Drive (not syncing)/Collection/ChatLikeQA/uploads/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "documents = loader.load()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 116.22it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 23.39it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 83.87it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.45it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 126.48it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 72.47it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 58.04it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 19.24it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 49.25it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 14.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 39.19it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 85.68it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 89.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.57it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 64.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 149.30it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 107.31it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 93.22it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 112.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 114.05it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 155.96it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "text_spliter = TokenTextSplitter(chuck_size=1000, chunk_overlap=20)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'chuck_size'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_spliter \u001b[39m=\u001b[39m TokenTextSplitter(chuck_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, chunk_overlap\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py:201\u001b[0m, in \u001b[0;36mTokenTextSplitter.__init__\u001b[0;34m(self, encoding_name, allowed_special, disallowed_special, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=192'>193</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=193'>194</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=194'>195</a>\u001b[0m     encoding_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=197'>198</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=198'>199</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=199'>200</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=200'>201</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=201'>202</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gyasis/miniconda3/envs/datascience/lib/python3.9/site-packages/langchain/text_splitter.py?line=202'>203</a>\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'chuck_size'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "text_spliter = TokenTextSplitter(chunk_size=1000, chunk_overlap=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    " docs = text_splitter.split_documents(documents)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_splitter' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_documents(documents)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_splitter' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "len(docs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(documents)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}